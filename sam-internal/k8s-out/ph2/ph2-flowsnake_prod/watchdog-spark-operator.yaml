apiVersion: v1
items:
- apiVersion: v1
  automountServiceAccountToken: true
  kind: ServiceAccount
  metadata:
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      manifestctl.sam.data.sfdc.net/swagger: disable
    name: watchdog-spark-operator-rolebinding
    namespace: flowsnake-watchdog
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: flowsnake-client-flowsnake-watchdog-Role
  subjects:
  - kind: ServiceAccount
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: v1
  data:
    analysis.py: |
      #!/usr/bin/env python
      # coding: utf-8

      """
      Runs the Spark Operator Watchdog script (e.g. watchdog-spark-on-k8s.sh). Performs analysis on the script's output to
      to classify failed runs by type of failure. Output and error code are then returned (with minimal additional decoration)
      to the caller (i.e. to the cliChecker Watchdog Go code). Additionally computes timing metrics (e.g. interval between
      Spark Driver requesting an exectuor and executor pod running). These timing metrics are reported for successful runs
      as well.

      The purpose of this program is to make it easier to determine which types of failures are the primary causes of
      script failures (and thus Flowsnake Service availability gaps). Determining what went wrong with a failed run requires
      carefully looking through the log, which was previously a tedious and manual process.

      Example: running analysis on test data:
      $ flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/watchdog-spark-on-k8s-analysis.py --test-dir flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/tests
      ✓ timeout_executor_slow_pod_creation_001.txt: TIMEOUT_EXECUTOR_SLOW_POD_CREATION
      ✓ driver_init_error_001.txt: DRIVER_INIT_ERROR
      ✓ etcd_no_leader_001.txt: {"class": "TIMEOUT_EXECUTOR_SLOW_POD_CREATION", "exception": "KubernetesClientException", "exception_cause": "ETCD_NO_LEADER"}
      ✓ exec_allocator_did_not_run_002.txt: EXECUTOR_ALLOCATOR_DID_NOT_RUN
      ✓ etcd_no_leader_002.txt: {"class": "SPARK_SUBMIT_FAILED", "exception": "KubernetesClientException", "exception_cause": "ETCD_NO_LEADER"}
      ✓ exec_allocator_did_not_run_001.txt: EXECUTOR_ALLOCATOR_DID_NOT_RUN
      ✓ timeout_executor_slow_pod_creation_002.txt: TIMEOUT_EXECUTOR_SLOW_POD_CREATION
      ✓ timeout_executor_slow_pod_creation_003.txt: TIMEOUT_EXECUTOR_SLOW_POD_CREATION
      ✓ timeout_exec_allocator_late_001.txt: TIMEOUT_EXECUTOR_ALLOCATOR_LATE
      ✓ scheduler_assume_pod_001.txt: SCHEDULER_ASSUME_POD

      Example: as above, but also writing metrics to Funnel:
      $ flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/watchdog-spark-on-k8s-analysis.py --test-dir flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/tests --metrics --sfdchosts /sfdchosts/hosts.json --watchdog-config /config/watchdog.json --host fs1shared0-flowsnakemastertest1-3-prd.eng.sfdc.net

      Example: as above, but writing to Funnel using defaults appropriate for local development:
      $ flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/watchdog-spark-on-k8s-analysis.py --test-dir flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/tests --metrics --dev

      Metrics written with the --dev flag can be found using Argus expressions
      GROUPBYTAG(-15m:sam.watchdog.CORP.NONE.flowsnake-local-test:cliChecker.SparkOperatorTest.FailureAnalysis:none, #class#, #exception#, #exception_cause#, #SUM#)
      or, to also group by app:
      GROUPBYTAG(-15m:sam.watchdog.CORP.NONE.flowsnake-local-test:cliChecker.SparkOperatorTest.FailureAnalysis:none, #app#, #class#, #exception#, #exception_cause#, #SUM#)
      and for timing:
      GROUPBYTAG(-15m:sam.watchdog.CORP.NONE.flowsnake-local-test:cliChecker.SparkOperatorTest.Times.*:none, #app#, #succeeded#, #AVERAGE#)

      The GROUPBYTAG facilitates display of optional tags per
      https://gus.lightning.force.com/lightning/r/0D5B000000sQcBnKAK/view

      Real results from live fleets can be found using Argus expressions
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none, #class#, #exception#, #exception_cause#, #SUM#)
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none, #app#, #class#, #exception#, #exception_cause#, #SUM#)
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*:none, #app#, #succeeded#, #AVERAGE#)

      Or to separate out estates and/or data centers, include the #estate# and/or #dc# tag in the grouping:
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none, #estate#, #class#, #exception#, #exception_cause#, #SUM#)

      To view all metric and tag permutations, omit grouping and aggregation:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none

      To view timing metrics:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*:avg
      Or, to separate out times per spark application:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*{app=*}:avg

      To view all metric and tag permutations, omit grouping and aggregation:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*:none

      TODO: Make a dashboard
      """

      from __future__ import print_function
      from argparse import ArgumentParser
      import calendar
      import os
      import re
      import subprocess
      import sys


      """
      Failure analysis metric. Recorded only when the result is a failure. Value always is 1. Metric tags indicate
      analysis of what went wrong.
      """
      FAILURE_ANALYSIS_METRIC_NAME = 'FailureAnalysis'
      #
      # Analysis dict keys
      # The result of the analysis is a string,string map, which is then sent to Argus as tags on a metric.
      CLASSIFICATION = 'class'  # Overall classification of the failure
      EXCEPTION = 'exception'  # Java class of most pertinent Exception, if any
      EXCEPTION_CAUSE = 'exception_cause'  # Determined cause of the Exception
      ANALYSIS_KEYS = [CLASSIFICATION, EXCEPTION, EXCEPTION_CAUSE]


      """
      Timing analysis metrics. Recorded whenever the data could be obtained, including for successful runs. Value is time in
      seconds. Each time has its own metric.
      """
      TIMING_METRIC_SUCCESS_TAG = 'succeeded'

      # Interval between the creating of the Spark Application and detecting the driver pod.
      TIMING_METRIC_NAME_DRIVER_POD_DETECTED = 'AppCreationToDriverPodDetected'
      # Interval between pending driver pod and scheduled driver pod. Only present when the driver got stuck pending instead of being directly created.
      TIMING_METRIC_NAME_DRIVER_POD_PENDING_DELAY = 'DriverPodSchedulingDelay'
      # Interval between driver pod scheduled and driver pod Running
      TIMING_METRIC_NAME_DRIVER_POD_INITIALIZATION = 'DriverPodInitialization'
      # Interval between driver pod Running and driver logging that the Spark App was submitted
      TIMING_METRIC_NAME_DRIVER_APP_SUBMIT = 'DriverPodAppSubmit'
      # Interval between driver pod logging that the Spark App was submitted and that the JAR was added. (Believe that JAR added is logged after all prep work prior to requesting executors has been completed)
      TIMING_METRIC_NAME_DRIVER_APP_LOAD = 'DriverPodAppLoad'
      # Interval between driver pod logging that an executors was requested and that it starts doing work
      TIMING_METRIC_NAME_EXECUTOR_WAIT_TOTAL = 'ExecutorTotalWait'
      # Interval between the driver requesting an executor and detecting the executor pod.
      TIMING_METRIC_NAME_EXEC_POD_DETECTED = 'ExecutorAllocatorToPodDetected'
      # Interval between pending executor pod and scheduled executor pod. Only present when the executor got stuck pending instead of being directly created.
      TIMING_METRIC_NAME_EXEC_POD_PENDING_DELAY = 'ExecutorPodSchedulingDelay'
      # Interval between executor pod scheduled and executor pod Running
      TIMING_METRIC_NAME_EXEC_POD_INITIALIZATION = 'ExecutorPodInitialization'
      # Interval between executor pod running and executor picking up work from the driver
      TIMING_METRIC_NAME_EXEC_REGISTRATION = 'ExecutorPodRegistration'
      # Interval between executor picking up work from the driver and job completion
      TIMING_METRIC_NAME_JOB_RUNTIME = 'JobRunTime'
      # Interval between job completion and watchdog script completion
      TIMING_METRIC_NAME_CLEAN_UP = 'CleanUp'


      TAG_APP = 'app'  # Name of the Spark Application. Same across concurrent watchdog instances. Roughly represents feature being tested.
      TAG_APP_ID = 'app_id'  # Id of the Spark Application. Unique across concurrent executions but recurring over time.
      TAG_ESTATE = 'estate'  # Estate this metric was emitted from ("pod" of the Argus scope)
      TAG_DC = 'dc'  # Datacenter this metric was emitted from

      # ------------ Funnel client code adapted from
      # https://git.soma.salesforce.com/monitoring/collectd-write_funnel_py/blob/18ef838f5a6221450e51ee2d7beb984adb0a3dc7/funnel_writer.py
      # ------------
      import httplib
      import time
      import json
      from os import R_OK, access
      from os.path import isfile, exists
      import collections
      import logging
      import socket

      MAX_TRIES = 10
      THIS_SCRIPT = os.path.basename(__file__)

      logging.basicConfig(
          level=logging.INFO,
          format='[%(asctime)s] %(filename)s:%(lineno)d %(levelname)5s - %(message)s'
      )

      Metric = collections.namedtuple(
          'Metric', ('service', 'name', 'value', 'timestamp', 'context', 'tags'))

      MetricContext = collections.namedtuple(
          'MetricContext', ('datacenter', 'superpod', 'pod', 'host'))


      class MetricEncoder(json.JSONEncoder):
          def encode(self, obj):
              if isinstance(obj, (list, tuple)):
                  batched_list = []
                  for o in obj:
                      batched_list.append(self._translate(o))
                  result = json.JSONEncoder.encode(self, batched_list)
              else:
                  result = json.JSONEncoder.encode(self, self._translate(obj))
              logging.debug('Encoded metric(s): %s' % result)
              return result

          @staticmethod
          def _translate(metric):
              assert isinstance(metric, Metric)
              all_tags = {}
              all_tags.update(metric.context._asdict())
              all_tags.update(metric.tags)
              metric_content = {
                  'service': metric.service,
                  'metricName': metric.name,
                  'metricValue': metric.value,
                  'timestamp': metric.timestamp,
                  'tags': all_tags,
              }
              return metric_content


      class FunnelException(Exception):
          pass


      class FunnelClient():
          def __init__(self, funnel_endpoint,
                       metric_scheme_fingerprint="AVG7NnlcHNdk4t_zn2JBnQ",
                       timeout_seconds=10,
                       funnel_debug=False,
                       cert_path=None,
                       key_path=None,
                       http_allowed=False,
                       ):
              self.funnel_endpoint = funnel_endpoint
              self.fingerprint = metric_scheme_fingerprint
              self.timeout = timeout_seconds
              self.debug = str(funnel_debug).lower()
              self.certpath = cert_path
              self.keypath = key_path
              self.https_allowed = http_allowed

              self.url = '/funnel/v1/publishBatch?avroSchemaFingerprint=%s&debug=%s' % (self.fingerprint, self.debug)
              self.has_certs = self.certpath and exists(self.certpath) and isfile(self.certpath) and \
                               access(self.certpath, R_OK) and \
                               self.keypath and exists(self.keypath) and isfile(self.keypath) and access(self.keypath, R_OK)

          def post_request(self, post_data, numberofmetrics, tries):
              try:
                  # We rely on the fact that in idb if the funnel-server doesn't have a port
                  # then it's using HTTPS, else HTTP
                  if ":" not in self.funnel_endpoint and self.has_certs and self.https_allowed:
                      connection = httplib.HTTPSConnection(self.funnel_endpoint,
                                                           key_file=self.keypath, cert_file=self.certpath,
                                                           timeout=self.timeout)
                  else:
                      connection = httplib.HTTPConnection(self.funnel_endpoint, timeout=self.timeout)

                  headers = {"Content-type": "application/json"}

                  connection.request('POST', self.url, post_data, headers)
                  response = connection.getresponse()
                  response_body = response.read()
                  logging.debug("POST %s -> %s (metricssize:%d merticsnum:%d tries:%d)" % (
                      self.funnel_endpoint, response_body, sys.getsizeof(post_data), numberofmetrics, tries))

                  if response.status != 200:
                      return False, response_body
                  return True, None
              except Exception as e:
                  return False, str(e)

          def publish_batch(self, metrics):
              sdata = json.dumps(metrics, cls=MetricEncoder)
              message = ''
              for retry in range(0, MAX_TRIES):
                  success, message = self.post_request(sdata, len(metrics), retry+1)
                  if success:
                      return True
              raise FunnelException(message)
      # --------- End adapted metrics code


      parser = ArgumentParser()
      mode_group = parser.add_mutually_exclusive_group(required=True)
      mode_group.add_argument("--command", action='store_true',
                          help="Spark Operator Watchdog script (and arguments) to execute")
      mode_group.add_argument("--analyze", dest="analyze",
                          help="Process the provided static content rather than executing a script")
      mode_group.add_argument("--test-dir", dest="test_dir",
                              help="Process all files in the provided directory as static content. First line of each file must be asserted result.")
      parser.add_argument("--sfdchosts",
                          help="Path of SAM sfdchosts file. Required for metrics generation")
      parser.add_argument("--watchdog-config",
                          help="Path of SAM Watchdog config file. Required for metrics generation")
      parser.add_argument("--hostname",
                          help="Override hostname to use when determining metrics configuration")
      parser.add_argument("--metrics", action='store_true',
                          help="If set, metrics will be written indicating the analysis result")
      parser.add_argument("--dev", action='store_true',
                          help="If set, metrics can be written without specifying --sfdchosts or --watchdog-config. Uses hard-coded PRD Funnel endpoint, dc:CORP, superpod:NONE, pod:flowsnake-local-test.")
      parser.add_argument("--estate",
                          help="Override estate (Argus pod) to use when determining metrics configuration. For use in combination with --dev")
      args, additional_args = parser.parse_known_args()

      simple_regex_tests = {
          # Driver pod's init container errors out. Cause TBD.
          'DRIVER_INIT_ERROR': re.compile(r'Pod change detected.*-driver changed to Init:Error'),
          # Scheduler bug in Kubernetes <= 1.9.7 that randomly prevents re-use of pod name. No longer expected because pod names are now unique.
          'SCHEDULER_ASSUME_POD': re.compile(r"FailedScheduling.*AssumePod failed: pod .* state wasn't initial but get assumed"),
          # This should be accompanied by a useful Exception
          'SPARK_CONTEXT_INIT_ERROR': re.compile(r'Error initializing SparkContext'),
          # This one might be due to IP exhaustion; need to check kubelet logs. https://salesforce.quip.com/i0ThASBMoHqf#VCTACATj2IO
          'DOCKER_SANDBOX': re.compile(r'Failed create pod sandbox'),
          'KUBECTL_MAX_TRIES_TIMEOUT': re.compile(r'Invocation \([0-9/]*\) of \[kubectl .*\] failed \(timed out \([0-9]*s\)\). Giving up.'),
          'DRIVER_EVICTED': re.compile(r'NodeControllerEviction.*node-controller.*Marking for deletion Pod .*-driver'),
          'MADKUB_INIT_EMPTY_DIR': re.compile(r'Error: failed to start container "madkub-init": .*kubernetes.io~empty-dir/datacerts'),
          'CONNECTION_REFUSED': re.compile(r'The connection to the server 10.254.208.1:443 was refused'),
          'NOT_LOGGED_IN_UNAUTHORIZED': re.compile(r'error: You must be logged in to the server \(Unauthorized\)'),
      }

      metrics_enabled = False
      if args.metrics:
          hostname = args.hostname if args.hostname else socket.gethostname()
          if args.dev:
              funnel_client = FunnelClient('ajna0-funnel1-0-prd.data.sfdc.net:80')
              estate = args.estate if args.estate else 'flowsnake-local-test'
              metric_context = MetricContext('CORP', 'NONE', estate, hostname)
              metrics_enabled = True
          elif not args.sfdchosts or not args.watchdog_config:
              logging.error("Cannot emit metrics: --sfdchosts and --watchdog-config are both required (or --dev)")
          else:
              if args.estate:
                  logging.error("Cannot specify estate except in combination with --dev")
              else:
                  try:
                      with open(args.sfdchosts) as f:
                          host_data = json.load(f)
                          try:
                              host_entry = next(e for e in host_data['hosts'] if e['hostname'] == hostname)
                              kingdom = host_entry['kingdom'].upper()
                              superpod = host_entry['superpod'].upper()
                              pod = host_entry['estate']
                          except StopIteration:
                              raise StandardError("Cannot emit metrics: host %s not found in sfdchosts" % hostname)
                      with open(args.watchdog_config) as f:
                          funnel_endpoint = json.load(f)['funnelEndpoint']
                      funnel_client = FunnelClient(funnel_endpoint)
                      metric_context = MetricContext(kingdom, superpod, pod, hostname)
                      metrics_enabled = True
                  except StandardError as e:
                      logging.exception("Cannot emit metrics: error parsing sfdchosts %s and watchdog-config %s",
                                        args.sfdchosts, args.watchdog_config)

      r_app_created = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - sparkapplication "(?P<app>.*)" created')
      r_driver_pod_creation_event = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver: (?P<state>.*) on host.*')
      r_driver_pod_creation_event_pending = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver: Pending on host.*')
      r_driver_pod_initializing = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver(:| changed to) (PodInitializing|Init)')
      r_driver_pod_running = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver(:| changed to) Running')
      r_driver_pod_completed = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver changed to Completed')
      r_spark_submit_failed = re.compile(r'failed to run spark-submit')
      r_driver_context_app_submitted = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*SparkContext.* - Submitted application')
      r_driver_context_jar_added = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*SparkContext.* - Added JAR file:')
      r_exec_allocator = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*ExecutorPodsAllocator.* - Going to request [0-9]* executors from Kubernetes')
      r_timeout = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Timeout reached. Aborting wait for SparkApplication .* even though in non-terminal state (?P<state>\w*)\.')
      #r_ driver_running_event = re.compile(r'SparkDriverRunning\s+([0-9]+)([sm])\s+spark-operator\s+Driver .* is running')
      r_exec_pod_creation_event = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+: (?P<state>.*) on host.*')
      r_exec_pod_creation_event_pending = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+: Pending on host.*')
      r_exec_pod_initializing = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+(:| changed to) (PodInitializing|Init)')
      r_exec_pod_running = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+ changed to Running.')
      r_exec_registered_time = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*KubernetesClusterSchedulerBackend.*Registered executor')
      r_job_finished = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*DAGScheduler.*Job 0 finished')
      r_complete = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - .*Completion of .* test')

      # Regex for fully-qualified Java class name https://stackoverflow.com/a/5205467/708883
      r_exception = re.compile(r'(?P<cause>(Caused by: )?)(?P<package>[a-zA-Z_$][a-zA-Z\d_$]*\.)*(?P<class>[a-zA-Z_$][a-zA-Z\d_$]*Exception): (?P<message>.*)')
      simple_regex_exception_messages = {
          # Driver pod's init container errors out. Cause TBD.
          'ETCD_NO_LEADER': re.compile(r'client: etcd member .* has no leader'),
          'BROKEN_PIPE': re.compile(r'Broken pipe'),
          'SPARK_ADMISSION_WEBHOOK': re.compile(r'failed calling admission webhook "webhook\.sparkoperator\.k8s\.io'),
          'REMOTE_CLOSED_CONNECTION': re.compile(r'Remote host closed connection'),
          'CONNECTION_RESET': re.compile(r'Connection reset'),
          'KUBEAPI_SHUTDOWN': re.compile(r'Apisever is shutting down'),
          'RBAC': re.compile(r'(?:role.rbac.authorization.k8s.io ".*" not found|forbidden: User ".*" cannot .* in the namespace)'),
      }

      app = None
      app_id = None

      def spark_log_time_to_epoch(spark_time):
          """
          Convert time format of Spark logs to unix epoch (seconds)
          :param spark_time: UTC formatted e.g. 2019-05-15 00:31:56
          :return: unix epoch in seconds
          """
          return calendar.timegm(time.strptime(spark_time, "%Y-%m-%d %H:%M:%S"))


      def compute_times(output, succeeded=False):
          """
          Calculates time intervals between events in provided output. Side effect: sets global app_id and app variables.
          :param output: Output from spark operator execution
          :param succeeded: Whether output represents a successful execution
          :return: (metric -> int (seconds) dictionary, regex -> epoch dictionary)
          """
          timings = {}  # interval name -> computed interval in seconds
          global app, app_id
          error_states = {'Terminating', 'Unknown', 'Error'}
          # The times are computed by using two regular expressions; one marks the start of the interval and
          # one marks the end of the interval.

          # interval name -> (start regex, end regex). This the blueprint of what is to be computed.
          time_regex = {
              TIMING_METRIC_NAME_DRIVER_POD_DETECTED: (r_app_created, r_driver_pod_creation_event),
              TIMING_METRIC_NAME_DRIVER_POD_PENDING_DELAY: (r_driver_pod_creation_event_pending, r_driver_pod_initializing),
              TIMING_METRIC_NAME_DRIVER_POD_INITIALIZATION: (r_driver_pod_initializing, r_driver_pod_running),
              TIMING_METRIC_NAME_DRIVER_APP_SUBMIT: (r_driver_pod_running, r_driver_context_app_submitted),
              TIMING_METRIC_NAME_DRIVER_APP_LOAD: (r_driver_context_app_submitted, r_driver_context_jar_added),
              TIMING_METRIC_NAME_EXECUTOR_WAIT_TOTAL: (r_exec_allocator, r_exec_registered_time),
              TIMING_METRIC_NAME_EXEC_POD_DETECTED: (r_exec_allocator, r_exec_pod_creation_event),
              TIMING_METRIC_NAME_EXEC_POD_PENDING_DELAY: (r_exec_pod_creation_event_pending, r_exec_pod_initializing),
              TIMING_METRIC_NAME_EXEC_POD_INITIALIZATION: (r_exec_pod_initializing, r_exec_pod_running),
              TIMING_METRIC_NAME_EXEC_REGISTRATION: (r_exec_pod_running, r_exec_registered_time),
              TIMING_METRIC_NAME_JOB_RUNTIME: (r_exec_registered_time, r_job_finished),
              TIMING_METRIC_NAME_CLEAN_UP: (r_job_finished, r_complete),
          }

          # regex -> (epoch, match). Time value found for each regex. Memoize because regexes are used multiple times.
          regex_results = {}
          for r1, r2 in time_regex.values():
              for r in [r1, r2]:
                  if r not in regex_results:
                      m = r.search(output)
                      if m:
                          # Not all log lines express time in the same format, so need multiple conversion rules
                          # to get to epoch. Presume regex group names are standardized.
                          match_groups = m.groupdict()
                          if 'epoch' in match_groups:
                              regex_results[r] = int(match_groups['epoch'])
                          elif 'spark_time' in match_groups:
                              regex_results[r] = spark_log_time_to_epoch(match_groups['spark_time'])
                          else:
                              log("Bug: regex {} is supposed to extract times but has no recognized group names. Matched {}.".format(
                                  r.pattern, m.group(0)))
                      else:
                          # Record explicit failure ot match so we don't try this regex again
                          regex_results[r] = None

          # compute intervals now that we have found all the times.
          for interval_name, (r_start, r_end) in time_regex.iteritems():
              epoch_start = regex_results.get(r_start)
              epoch_end = regex_results.get(r_end)
              if epoch_start and epoch_end:
                  timings[interval_name] = epoch_end - epoch_start

          # Identify app creation
          m_app_created = r_app_created.search(output)
          if m_app_created:
              app_id = m_app_created.group('app')
              if app_id:
                  app = '-'.join(app_id.split('-')[0:-1])  # Assume app-name-uniqueid format

          if metrics_enabled:
              emit_timing_metrics(timings, succeeded)
          return (timings, regex_results)


      def add_standard_tags(tags):
          if app_id:
              tags[TAG_APP_ID] = app_id
          if app:
              tags[TAG_APP] = app
          tags[TAG_ESTATE] = metric_context.pod
          tags[TAG_DC] = metric_context.datacenter
          return tags


      def emit_timing_metrics(times, succeeded):
          tags = add_standard_tags({
              TIMING_METRIC_SUCCESS_TAG: "OK" if succeeded else "FAIL"
          })
          m_list = [
              Metric('sam.watchdog', ['cliChecker', 'SparkOperatorTest', 'Times', metric], seconds, int(time.time()), metric_context, tags)
              for metric, seconds in times.iteritems()]
          try:
              funnel_client.publish_batch(m_list)
          except Exception as e:
              logging.exception('Failed to send %d metrics to funnel' % len(m_list))


      def analyze_helper(output, timings):
          """
          Classifies failure in provided output
          :param output: output from failed spark operator execution
          :param timings: metric -> int (seconds) dictionary. See compute_times
          :return: class (as string)
          """
          for code, regex in simple_regex_tests.iteritems():
              if regex.search(output):
                  return code

          # Check for termination due to timeout.
          m_timeout = r_timeout.search(output)
          if m_timeout:
              # We have observed the Spark Application state failing to update even though e.g. the driver runs to
              # completion.
              m_completed = r_driver_pod_completed.search(output)
              if m_completed and int(m_completed.group('epoch')) < int(m_timeout.group('epoch')):
                  # Driver pod completed before the timeout was reached.
                  return 'TIMEOUT_SPARK_APP_NOT_COMPLETING'
              if m_timeout.group('state') in {'RUNNING', 'SUBMITTED'}:
                  # TODO: Look at collected metrics for a better sense of what normal values are.
                  # timing -> (max_seconds, classification_if_exceeded)
                  thresholds = {
                      TIMING_METRIC_NAME_DRIVER_POD_DETECTED: (60, 'TIMEOUT_DRIVER_SLOW_POD_CREATION'),
                      TIMING_METRIC_NAME_DRIVER_POD_PENDING_DELAY: (60, 'TIMEOUT_DRIVER_SLOW_POD_SCHEDULING'),
                      TIMING_METRIC_NAME_DRIVER_POD_INITIALIZATION: (60, 'TIMEOUT_DRIVER_SLOW_POD_INIT'),
                      TIMING_METRIC_NAME_DRIVER_APP_SUBMIT: (60, 'TIMEOUT_DRIVER_SLOW_APP_SUBMIT'),
                      TIMING_METRIC_NAME_DRIVER_APP_LOAD: (60, 'TIMEOUT_DRIVER_SLOW_APP_LOAD'),
                      TIMING_METRIC_NAME_EXEC_POD_DETECTED: (30, 'TIMEOUT_EXECUTOR_SLOW_POD_CREATION'),
                      TIMING_METRIC_NAME_EXEC_POD_PENDING_DELAY: (60, 'TIMEOUT_EXECUTOR_SLOW_POD_SCHEDULING'),
                      TIMING_METRIC_NAME_EXEC_POD_INITIALIZATION: (30, 'TIMEOUT_EXECUTOR_SLOW_POD_INIT'),
                      TIMING_METRIC_NAME_EXEC_REGISTRATION: (30, 'TIMEOUT_EXECUTOR_SLOW_REGISTRATION'),
                      TIMING_METRIC_NAME_JOB_RUNTIME: (30, 'TIMEOUT_SLOW_JOB_RUN'),
                      TIMING_METRIC_NAME_CLEAN_UP: (30, 'TIMEOUT_SLOW_CLEANUP'),
                  }
                  for metric, seconds in timings.iteritems():
                      if metric in thresholds:
                          threshold, classification = thresholds[metric]
                          if seconds >= threshold:
                              return classification
              return 'TIMEOUT_' + (m_timeout.group('state') or 'WITHOUT_STATE')
          else:
              # Failure was *not* due to a timeout.
              if r_spark_submit_failed.search(output):
                  return "SPARK_SUBMIT_FAILED"  # Exception classification will provide reason
              else:
                  return "UNRECOGNIZED_NON_TIMEOUT"


      def detect_exceptions(output):
          """
          Identifies noteworthy exceptions in provided output
          :param output: output from failed spark operator execution
          :return: class (as string)
          """
          exception = {}
          match_iterator = r_exception.finditer(output)
          # heuristic: assume that the first Exception is the most interesting, but if it has logged "Caused by" lines, use
          # the final reported cause. E.g. BazException is selected from the following:
          # FooException: foo
          #    ...
          # Caused by BarException: bar
          #    ...
          # Caused by BazException: baz
          #    ...
          # ...
          # QuuxException: quux
          #    ...
          # Caused by FnarfException: fnarf
          #    ...
          exception_match = None
          for m in match_iterator:
              if not exception_match:
                  # First exception found is better than no exception
                  exception_match = m
              elif m.group('cause'):
                  # If this is a cause, prefer it over the previously found exception

                  # Exception (ha!) to the rule: sometimes the cause is less specific. Don't prefer it in that case. E.g.
                  # javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake
                  # Caused by: java.io.EOFException: SSL peer shut down incorrectly
                  # "Remote host closed connection" actually seems more useful.
                  key_is_more_specific_than = {
                      'SSLHandshakeException': 'EOFException',
                      'SocketTimeoutException': 'SocketException',
                  }
                  if key_is_more_specific_than.get(exception_match.group('class')) == m.group('class'):
                      continue
                  else:
                      exception_match = m
              else:
                  # Otherwise we're done; subsequent exception blocks are probably just cascading errors
                  # Exception (ha!) to the rule: sometimes the first exception logged has fewer details.
                  key_is_more_specific_than = {
                      'KubernetesClientException': 'ProtocolException',
                  }
                  if key_is_more_specific_than.get(m.group('class')) == exception_match.group('class'):
                      exception_match = m
                  else:
                      break

          if exception_match:
              # Record the exception itself
              exception[EXCEPTION] = exception_match.group('class')
              # Record additional exception classification based on the message
              for code, regex in simple_regex_exception_messages.iteritems():
                  if regex.search(exception_match.group('message')):
                      exception[EXCEPTION_CAUSE] = code
          return exception


      def analyze(output, timings):
          """
          Classifies failure in provided output, identifies noteworthy exceptions, and emits metrics (if enabled)
          :param output: output from failed spark operator execution
          :param timings: metric -> int (seconds) dictionary. See compute_times
          :return: class (as string)
          """
          analysis = {
              CLASSIFICATION: analyze_helper(output, timings)
          }
          analysis.update(detect_exceptions(output))
          if metrics_enabled:
              emit_failure_analysis_metrics(analysis)
          return analysis


      def emit_failure_analysis_metrics(analysis):
          # Although the GROUPBY magic as described in the example metric queries can work around optional tags, it seems
          # expedient to just always populate the tags to prevent Argus surprises in the future.
          tags = add_standard_tags(dict([(key, analysis.get(key, "None")) for key in ANALYSIS_KEYS]))
          m_list = [
              Metric('sam.watchdog', ['cliChecker', 'SparkOperatorTest', FAILURE_ANALYSIS_METRIC_NAME], 1, int(time.time()), metric_context, tags)
          ]
          try:
              funnel_client.publish_batch(m_list)
          except Exception as e:
              logging.exception('Failed to send %d metrics to funnel' % len(m_list))


      def pretty_result(analysis):
          # Convert result to format used in the test files
          # Use bare classification if there is no additional info. Otherwise string representation of named tuple.
          return json.dumps(analysis, sort_keys=True) if len(analysis) > 1 else analysis[CLASSIFICATION]


      def log(s):
          print('+++ [{}] {}'.format(THIS_SCRIPT, s))


      if args.command:
          start = time.time()
          try:
              log("Executing and analyzing output of: {}".format(" ".join(additional_args)))
              output = subprocess.check_output(additional_args, stderr=subprocess.STDOUT)
              print(output, end='')
              timings, epochs = compute_times(output, succeeded=True)
              log("Times: ")
              log("No errors ({}s)".format(int(time.time() - start)))
              sys.exit(0)
          except subprocess.CalledProcessError as e:
              print(e.output, end='')
              timings, epochs = compute_times(e.output)
              log("Analysis of failure [{}] ({}s): {}".format(
                  e.returncode,
                  int(time.time() - start),
                  pretty_result(analyze(e.output, timings))))
              log("Times: {}".format(json.dumps(timings, sort_keys=True)))
              sys.exit(e.returncode)
      elif args.analyze:
          with open(args.analyze, 'r') as file:
              output = file.read()
              timings, epochs = compute_times(output)
              print(pretty_result(analyze(output, timings)))
              print("Times: {}".format(json.dumps(timings, sort_keys=True)))
      elif args.test_dir:
          success = True
          for filename in sorted(os.listdir(args.test_dir)):
              with open(os.path.join(args.test_dir, filename), 'r') as file:
                  expect, output = file.read().split('\n', 1)
                  timings, epochs = compute_times(output)
                  text_result = pretty_result(analyze(output, timings))
                  if text_result == expect:
                      print(u"\u2713 {}: {}".format(filename, expect).encode('utf-8'))
                  else:
                      print(u"\u2718 {}: {} expected, {} obtained".format(filename, expect, text_result).encode('utf-8'))
                      success = False
                  # Too much visual noise. But occasionally useful during development.
                  # print("Times: {}".format(json.dumps(timings, sort_keys=True)))
          if not success:
              sys.exit(1)
      else:
          log("Bug: argparse failed to set a known operation mode.")
          sys.exit(1)
    check-impersonation.sh: |
      #!/usr/bin/bash

      # This test actually does not involve Spark applications at all, but it is part of verifying the Flowsnake v2 offering.
      # This test performs a minimal interaction with the Kubernetes API to verify connectivity, authentication, and
      # authorization.

      KUBECONFIG="$1"

      # Success of this command demonstrates successful connection via impersonation proxy and mapping to
      # user account flowsnake_test.flowsnake-watchdog (which in turn is bound to flowsnake-client-flowsnake-watchdog-Role)
      # (Success does not depend on whether there exist any sparkapplication resources in the namespace)
      kubectl -n ${2:-flowsnake-watchdog} get sparkapplications
    check-spark-operator.sh: |
      #!/usr/bin/bash
      set -o nounset
      set -o errexit
      set -o pipefail

      # Disable use of SAM's custom kubeconfig, restore default Kubernetes behavior (this cluster's kubeapi using service account token)
      unset KUBECONFIG

      NAMESPACE=flowsnake-watchdog
      KUBECTL_TIMEOUT_SECS=10
      # Give kubeapi 1 minute to recover. 10 second timeout, 7th request begins 60s after 1st.
      KUBECTL_ATTEMPTS=7

      # Parse command line arguments. https://stackoverflow.com/a/14203146
      POSITIONAL=()
      while [[ $# -gt 0 ]]
      do
      key="$1"

      case $key in
          --kubeconfig)
          # Use a custom kubeconfig (e.g. to access via MadDog PKI certs and Impersonation Proxy)
          export KUBECONFIG="$2"
          shift # past argument
          shift # past value
          ;;
          --namespace)
          # Use a custom namespace (default is flowsnake-watchdog)
          export NAMESPACE="$2"
          shift # past argument
          shift # past value
          ;;
          --kubectl-timeout)
          # Specify timeout (seconds) for individual kubectl invocations (default is 5)
          export KUBECTL_TIMEOUT_SECS="$2"
          shift # past argument
          shift # past value
          ;;
          --kubectl-attempts)
          # Specify number of attempts for individual kubectl invocations (default is 3)
          export KUBECTL_ATTEMPTS="$2"
          shift # past argument
          shift # past value
          ;;
          *)    # unknown option
          POSITIONAL+=("$1") # save it in an array for later
          shift # past argument
          ;;
      esac
      done
      set -- "${POSITIONAL[@]}" # restore positional parameters

      TEST_RUNNER_ID=${TEST_RUNNER_ID:-$(cut -c1-8 < /proc/sys/kernel/random/uuid)}

      # Check if spec is a jsonnet template
      if [[ ".jsonnet" == "${1: -8}" ]] ; then
          SPEC_INPUT=$(basename "$1")
          # Replace .jsonnet with -<ID>.json to get output filename
          SPEC_PATH=/strata-test-specs-out/${SPEC_INPUT%%.*}-${TEST_RUNNER_ID}.json
          jsonnet -V imageRegistry=${DOCKER_REGISTRY} -V jenkinsId=${TEST_RUNNER_ID} -V dockerTag=${DOCKER_TAG} -V s3ProxyHost=${S3_PROXY_HOST} -V driverServiceAccount=${DRIVER_SERVICE_ACCOUNT} ${1} | \
          python -c 'import json,sys; j=json.load(sys.stdin); j_clean=j if len(j.keys())>1 else j[j.keys()[0]]; print json.dumps(j_clean, indent=4)' > ${SPEC_PATH}
          if [ -f "${SPEC_PATH}" ]; then
              SPEC="${SPEC_PATH}"
          else
              echo "spec ${SPEC_PATH} doesn't exist."
              exit 1
          fi
      else
          # regular spec
          SPEC=$1
      fi

      APP_NAME=$(python -c 'import json,sys; print json.load(sys.stdin)["metadata"]["name"]' < $SPEC)
      SELECTOR="sparkoperator.k8s.io/app-name=$APP_NAME"
      # Exit after 5 minutes to ensure we exit before cliChecker kills us (10 mins) so that all output can be logged.
      TIMEOUT_SECS=$((60*5))

      # output Unix time to stdout
      epoch() {
          date '+%s'
      }
      START_TIME=$(epoch)

      # Format string for log output by decorating with date, time, app name
      format() {
          sed -e "s/^/$(date +'%m%d %H:%M:%S') [$(epoch)] $APP_NAME - /"
      }

      # Format and output provided string to stdout
      log() {
          if [[ "$@" != "" ]]; then
              echo "${@}" | format
          fi
      }

      # Format (with heading marker) and output provided string to stdout
      log_heading() {
          log "======== $@ ========"
      }

      # Format (with sub-heading marker) and output provided string to stdout
      log_sub_heading() {
          log "---- $@ ----"
      }

      # Run kubectl in namespace.
      # Use for extracting programatic values; otherwise prefer kcfw_log for formatted output.
      #
      # stdout is printed without change.
      # stderr is log-formatted and printed.
      #
      # Operations are timed out after KUBECTL_TIMEOUT_SECS and retried KUBECTL_ATTEMPTS times upon timeout or non-zero exit
      # Timeout and retry events are printed to stderr
      kcfw() {
          ATTEMPT=1
          while true; do
              # In addition to the timeout for this specific kubectl command, we need to check that the script hasn't
              # passed its overall timeout.
              EPOCH=$(epoch)
              stdout=$(mktemp /tmp/$(basename $0)-stdout.XXXXXX)
              stderr=$(mktemp /tmp/$(basename $0)-stderr.XXXXXX)
              # Capture result code, don't trigger errexit. https://stackoverflow.com/a/15844901
              timeout --signal=9 ${KUBECTL_TIMEOUT_SECS} kubectl -n ${NAMESPACE} "$@" 2>${stderr} >${stdout} && RESULT=$? || RESULT=$?
              # Hack to simplify scripting: if you try to delete something and get back a NotFound, treat that as a success.
              if [[ $(echo "$@" | grep -P '\bdelete\b') && $(grep -P '\(NotFound\).* not found' ${stderr}) ]]; then
                  return 0
              fi
              # Hack to simplify scripting: 'No resources found' is never useful.
              # Goofy: get with a selector says "No resources found." on stderr but delete says "No resources found" on stdout.
              sed -i '/^No resources found\.\?$/d' ${stdout}
              sed -i '/^No resources found\.\?$/d' ${stderr}
              # Format captured stderr for logging and output it to stderr
              cat ${stderr} | format >&2
              rm ${stderr}
              cat ${stdout}
              rm ${stdout}
              if [[ $RESULT == 0 ]]; then
                  # Success! We're done.
                  return $RESULT;
              fi;
              MSG="Invocation ($ATTEMPT/$KUBECTL_ATTEMPTS) of [kubectl -n ${NAMESPACE} $@] failed ($(if (( $RESULT == 124 || $RESULT == 137 )); then echo "timed out (${KUBECTL_TIMEOUT_SECS}s)"; else echo $RESULT; fi))."
              if (( EPOCH - START_TIME >= TIMEOUT_SECS )); then
                  log "$MSG Out of time. Giving up." >&2
                  return ${RESULT}
              elif (( $ATTEMPT < $KUBECTL_ATTEMPTS )); then
                  log "$MSG Will sleep $KUBECTL_TIMEOUT_SECS seconds and then try again." >&2
                  sleep ${KUBECTL_TIMEOUT_SECS}
              else
                  log "$MSG Giving up." >&2
                  return ${RESULT}
              fi;
              ATTEMPT=$(($ATTEMPT + 1))
          done;
      }

      # Like kcfw, plus also apply log formatting to stdout.
      kcfw_log() {
        # pipefail is set, so sed won't lose any failure exit code returned by kubectl
        # stderr is already formatted by kcfw, so only need to add formatting to stdout
        kcfw "$@" | format
      }

      # Extract the "Events" section from a kubectl description of a resource.
      events() {
          log_sub_heading "Begin Events"
          # awk magic prints only the Name: line and the Events lines (terminated by a blank line).
          # Use kcfw and explicitly call format after so Awk can look for start-of-line.
          kcfw describe sparkapplication $APP_NAME | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)' | format
          kcfw describe pod -l ${SELECTOR},spark-role=driver | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)' | format
          kcfw describe pod -l ${SELECTOR},spark-role=executor | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)' | format
          log_sub_heading "End Events"
      }

      # Return the state of the Spark application.
      # Terminal values are COMPLETED and FAILED https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#the-crd-controller
      state() {
          kcfw get sparkapplication $APP_NAME -o jsonpath='{.status.applicationState.state}'
      }

      # Output logs for specified pod to stdout
      # Future: Alternatively, generate a Splunk link?
      declare -A POD_LOGS_COLLECTED # pod name -> "true" if logs for pod were already collected
      pod_log() {
          # Checking for key with set -o nounset active: https://stackoverflow.com/a/13221491
          if [[ -z "${POD_LOGS_COLLECTED[$1]+present}" ]]; then
              log_sub_heading "Begin $1 Log"
              # || true to avoid failing script if pod has gone away.
              kcfw logs $1 || true
              log_sub_heading "End $1 Log"
              POD_LOGS_COLLECTED["$1"]="true"
          fi
      }

      # Log changes to pods spawned for SparkApplication
      declare -A PREVIOUS_POD_REPORTS # pod name -> "<pod_status> on host <nodeName>"
      report_pod_changes() {
          unset POD_REPORTS
          declare -A POD_REPORTS # pod name -> "<pod_status> on host <nodeName>"
          # Fetch pod names and their status for this SparkApplication
          # Note that the status from kubectl get contains a more informative single-term status than is available in the JSON.
          # The JSON contains the phase (Pending -> Running -> Completed), which does not mention Init, and the detailed
          # conditions and containerStatuses lists, which are difficult to summarize.
          # Relevant pods for our spark application have label metadata.labels.spark-app-selector=$APP_ID
          # Reading command output line by line: https://unix.stackexchange.com/a/52027
          while read POD_REPORT; do
              POD_NAME=$(echo $POD_REPORT | cut -d' ' -f1)
              REPORT=$(echo $POD_REPORT | cut -d' ' -f1 --complement)
              POD_REPORTS["$POD_NAME"]="${REPORT}"
          done < <(kcfw get pods -l${SELECTOR} --show-all -o wide --no-headers | awk '{print $1, $3, "on host", $7}')

          # Note: Initially used process substitution (as in FOO=$(comm <(...) <(...) )here, but that left defunct grandchild
          # processes behind. Switch to temp files instead.
          # (Not totally clear on why. Something like: process substitution never collects the exit status of the invoked
          # process. Thus when the Bash script exits, the process gets re-parented to the cliChecker. But the cliChecker
          # does not collect it either, so zombie process entries pile up.)
          PREVIOUS_POD_NAMES_FILE=$(mktemp /tmp/$(basename $0)-previous-pods.XXXXXX)
          CURRENT_POD_NAMES_FILE=$(mktemp /tmp/$(basename $0)-current-pods.XXXXXX)
          # Write out the names of the pods. ${!MY_MAP[@]} yields the keys of the associative array
          echo ${!PREVIOUS_POD_REPORTS[@]} | xargs -n1 | sort > "$PREVIOUS_POD_NAMES_FILE"
          echo ${!POD_REPORTS[@]} | xargs -n1 | sort > "$CURRENT_POD_NAMES_FILE"
          # Compare pod names from before with ones present now.
          # Bash array set operations: See https://unix.stackexchange.com/a/104848
          REMOVED_POD_NAMES=$(comm -23 "$PREVIOUS_POD_NAMES_FILE" "$CURRENT_POD_NAMES_FILE")
          NEW_POD_NAMES=$(comm -13 "$PREVIOUS_POD_NAMES_FILE" "$CURRENT_POD_NAMES_FILE")
          EXISTING_POD_NAMES=$(comm -12 "$PREVIOUS_POD_NAMES_FILE" "$CURRENT_POD_NAMES_FILE")
          rm "$PREVIOUS_POD_NAMES_FILE"
          rm "$CURRENT_POD_NAMES_FILE"

          # Can't simply copy associative arrays in bash, so perform maintenance on PREVIOUS_POD_REPORTS as we go.
          for POD_NAME in ${REMOVED_POD_NAMES}; do
              log "Pod change detected: ${POD_NAME} removed."
              unset PREVIOUS_POD_REPORTS["$POD_NAME"]
          done
          for POD_NAME in ${NEW_POD_NAMES}; do
              log "Pod change detected: ${POD_NAME}: ${POD_REPORTS["${POD_NAME}"]}.";
              PREVIOUS_POD_REPORTS["$POD_NAME"]=${POD_REPORTS["${POD_NAME}"]}
          done;
          for POD_NAME in ${EXISTING_POD_NAMES}; do
              # The hostname won't change, so only report the pod status. ${VAR%% *} means delete everything after the first
              # space. Thus "<pod_status> on host <nodeName>" becomes "<pod_status>"
              # http://tldp.org/LDP/abs/html/string-manipulation.html
              # Except if the previous status was 'Pending on host <none>', in which case this is the first opportunity to log
              # the host name.
              OLD_REPORT="${PREVIOUS_POD_REPORTS[${POD_NAME}]}"
              NEW_REPORT="${POD_REPORTS[${POD_NAME}]}"
              if [[ "${OLD_REPORT}" != "${NEW_REPORT}" ]]; then
                  if [[ ${OLD_REPORT} == *"on host <none>" ]] && [[ ${NEW_REPORT} != *"on host <none>" ]]; then
                      REPORT_DISPLAY="${NEW_REPORT}"
                  else
                      REPORT_DISPLAY="${NEW_REPORT%% *}"
                  fi
                  log "Pod change detected: ${POD_NAME} changed to ${REPORT_DISPLAY} (previously ${OLD_REPORT%% *})."
                  PREVIOUS_POD_REPORTS["$POD_NAME"]="${NEW_REPORT}"
                  if [[ "${NEW_REPORT%% *}" == "Completed" ]] || [[ "${NEW_REPORT%% *}" == "Error" ]] || [[ "${NEW_REPORT%% *}" == "Terminating" ]]; then
                      # Grab the logs now rather than waiting until the end of the script; the pod might be deleted by then.
                      pod_log ${POD_NAME}
                  fi
              fi
          done;
      }


      # ------ Initialize ---------
      log_heading "Beginning $APP_NAME test"
      # Sanity-check kubeapi connectivity
      kcfw_log cluster-info

      # ------ Clean up prior runs ---------
      log "Cleaning up SparkApplication/Pod older than 1 hours from prior runs."
      # https://stackoverflow.com/questions/48934491/kubernetes-how-to-delete-pods-based-on-age-creation-time
      APPS=$(kcfw get sparkapplication -o go-template='{{range .items}}{{.metadata.name}} {{.metadata.creationTimestamp}}{{"\n"}}{{end}}' | awk '$2 <= "'$(date -d'now-1 hours' -Ins --utc | sed 's/+0000/Z/')'" { print $1 }')
      for APP in ${APPS}; do
          PODSELECTOR="sparkoperator.k8s.io/app-name=${APP}"
          kcfw_log delete sparkapplication ${APP}
          kcfw_log delete pod -l ${PODSELECTOR}
      done

      # ------ Run ---------
      log "Creating SparkApplication $APP_NAME"
      kcfw_log create -f $SPEC
      SPARK_APP_START_TIME=$(epoch)

      # If we've gotten this far, we'd like to collect as much forensic data as possible
      set +o errexit

      LAST_LOGGED=$(epoch)
      log "Waiting for SparkApplication $APP_NAME to reach a terminal state."
      STATE=$(state)
      while true; do
          EPOCH=$(epoch)
          if $(echo ${STATE} | grep -P '(COMPLETED|FAILED)' > /dev/null); then
              log "SparkApplication $APP_NAME has terminated after $(($EPOCH - $SPARK_APP_START_TIME)) seconds. State is $STATE."
              break
          fi
          # Use start time of script for timeout computation in order to still exit in timely fashion even if setup was slow
          if (( EPOCH - START_TIME >= TIMEOUT_SECS )); then
              log "Timeout reached. Aborting wait for SparkApplication $APP_NAME even though in non-terminal state $STATE."
              break
          fi
          if (( EPOCH - LAST_LOGGED > 60 )); then
              log "...still waiting for terminal state (currently $STATE) after $((EPOCH-SPARK_APP_START_TIME)) seconds.";
              events;
              LAST_LOGGED=${EPOCH}
          fi;
          report_pod_changes
          sleep 1;
          STATE=$(state)
      done;
      EXIT_CODE=$(echo ${STATE} | grep COMPLETED > /dev/null; echo $?)

      # ------ Report Results ---------
      report_pod_changes
      events

      POD_NAME=$(kcfw get pod -l ${SELECTOR},spark-role=driver -o name)
      if [[ -z ${POD_NAME} ]]; then
          log "Cannot locate driver pod. Maybe it never started? No logs to display."
      else
          pod_log ${POD_NAME}
      fi

      log -------- Executor Pods ----------
      EXECUTOR_PODS=$(kcfw get pod -l ${SELECTOR},spark-role=executor -o name)
      for POD_NAME in ${EXECUTOR_PODS}; do
          pod_log ${POD_NAME}
      done;

      if $(echo ${STATE} | grep -P '(COMPLETED|FAILED)' > /dev/null); then
          # Delete so that Kubernetes is in a cleaner state when the next test execution starts
          log "Cleaning up stuff for completed or failed test."
          kcfw_log delete sparkapplication ${APP_NAME}
          kcfw_log delete pod -l ${SELECTOR}
      fi

      log_heading "Completion of $APP_NAME test, returning $EXIT_CODE"
      exit ${EXIT_CODE}
    kubeconfig-impersonation-proxy: |
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority: /certs/ca.pem
          server: https://kubernetes-api-flowsnake-ph2.slb.sfdc.net
        name: kubernetes
      contexts:
      - context:
          cluster: kubernetes
          user: kubernetes
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: kubernetes
        user:
          client-certificate: /certs/client/certificates/client.pem
          client-key: /certs/client/keys/client-key.pem
  kind: ConfigMap
  metadata:
    name: watchdog-spark-on-k8s-script-configmap
    namespace: flowsnake
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    labels:
      name: watchdog-spark-operator
    name: watchdog-spark-operator
    namespace: flowsnake
  spec:
    replicas: 10
    selector:
      matchLabels:
        app: watchdog-spark-operator
        apptype: monitoring
    template:
      metadata:
        annotations:
          madkub.sam.sfdc.net/allcerts: '{"certreqs": [{"cert-type": "client", "kingdom":
            "ph2", "name": "watchdogsparkoperator", "role": "flowsnake_test.flowsnake-watchdog"}]}'
        labels:
          app: watchdog-spark-operator
          apptype: monitoring
          flowsnakeOwner: dva-transform
          flowsnakeRole: WatchdogSparkOperator
      spec:
        containers:
        - command:
          - /sam/watchdog
          - -role=CLI
          - -emailFrequency=1h
          - -timeout=2s
          - -funnelEndpoint=ajna0-funnel1-0-ph2.data.sfdc.net:80
          - --config=/config/watchdog.json
          - -cliCheckerCommandTarget=SparkOperatorTest
          - --hostsConfigFile=/sfdchosts/hosts.json
          - -watchdogFrequency=1m
          - -alertThreshold=1m
          - -cliCheckerTimeout=6m
          - -includeCommandOutput=true
          env:
          - name: DOCKER_TAG
            value: "4"
          - name: S3_PROXY_HOST
            value: public0-proxy1-0-ph2.data.sfdc.net
          - name: DRIVER_SERVICE_ACCOUNT
            value: spark-driver-flowsnake-watchdog
          - name: DOCKER_REGISTRY
            value: ops0-artifactrepo1-0-ph2.data.sfdc.net
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: ops0-artifactrepo1-0-ph2.data.sfdc.net/dva/flowsnake-spark-on-k8s-integration-test-runner:4
          imagePullPolicy: IfNotPresent
          name: watchdog
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: "1"
              memory: 1Gi
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /sfdchosts
            name: sfdchosts
          - mountPath: /watchdog-spark-scripts
            name: watchdog-spark-scripts
          - mountPath: /certs
            name: datacerts
        - args:
          - /sam/madkub-client
          - --madkub-endpoint
          - https://10.254.208.254:32007
          - --maddog-endpoint
          - https://all.pkicontroller.pki.blank.ph2.prod.non-estates.sfdcsd.net:8443
          - --maddog-server-ca
          - /etc/pki_service/ca/security-ca.pem
          - --madkub-server-ca
          - /etc/pki_service/ca/cacerts.pem
          - --token-folder
          - /tokens
          - --kingdom
          - ph2
          - --superpod
          - None
          - --estate
          - ph2-flowsnake_prod
          - --refresher
          - --run-init-for-refresher-mode
          - --cert-folders
          - watchdogsparkoperator:/certs
          - --ca-folder
          - /etc/pki_service/ca
          - --funnel-endpoint
          - http://ajna0-funnel1-0-ph2.data.sfdc.net:80
          env:
          - name: MADKUB_NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MADKUB_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MADKUB_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ops0-artifactrepo1-0-ph2.data.sfdc.net/tnrp/sam/madkub:1.0.0-0000084-9f4a6ca6
          name: sam-madkub-integration-refresher
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 7337
          volumeMounts:
          - mountPath: /certs
            name: datacerts
          - mountPath: /tokens
            name: tokens
          - mountPath: /etc/pki_service/ca
            name: certificate-authority
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/certificates
            name: client-certificate
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/keys
            name: client-key
            readOnly: true
        hostNetwork: true
        initContainers:
        - args:
          - /sam/madkub-client
          - --madkub-endpoint
          - https://10.254.208.254:32007
          - --maddog-endpoint
          - https://all.pkicontroller.pki.blank.ph2.prod.non-estates.sfdcsd.net:8443
          - --maddog-server-ca
          - /etc/pki_service/ca/security-ca.pem
          - --madkub-server-ca
          - /etc/pki_service/ca/cacerts.pem
          - --token-folder
          - /tokens
          - --kingdom
          - ph2
          - --superpod
          - None
          - --estate
          - ph2-flowsnake_prod
          - --cert-folders
          - watchdogsparkoperator:/certs
          - --ca-folder
          - /etc/pki_service/ca
          - --funnel-endpoint
          - http://ajna0-funnel1-0-ph2.data.sfdc.net:80
          env:
          - name: MADKUB_NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MADKUB_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MADKUB_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ops0-artifactrepo1-0-ph2.data.sfdc.net/tnrp/sam/madkub:1.0.0-0000084-9f4a6ca6
          name: sam-madkub-integration-init
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 7337
          volumeMounts:
          - mountPath: /certs
            name: datacerts
          - mountPath: /tokens
            name: tokens
          - mountPath: /etc/pki_service/ca
            name: certificate-authority
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/certificates
            name: client-certificate
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/keys
            name: client-key
            readOnly: true
        restartPolicy: Always
        serviceAccount: watchdog-spark-operator-serviceaccount
        serviceAccountName: watchdog-spark-operator-serviceaccount
        volumes:
        - configMap:
            name: watchdog
          name: config
        - configMap:
            defaultMode: 493
            name: watchdog-spark-on-k8s-script-configmap
          name: watchdog-spark-scripts
        - configMap:
            name: sfdchosts
          name: sfdchosts
        - emptyDir:
            medium: Memory
          name: datacerts
        - emptyDir:
            medium: Memory
          name: tokens
        - hostPath:
            path: /etc/pki_service/ca
          name: certificate-authority
        - hostPath:
            path: /etc/pki_service/platform/platform-client/certificates
          name: client-certificate
        - hostPath:
            path: /etc/pki_service/platform/platform-client/keys
          name: client-key
kind: List
metadata: {}
