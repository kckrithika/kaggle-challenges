apiVersion: v1
items:
- apiVersion: v1
  automountServiceAccountToken: true
  kind: ServiceAccount
  metadata:
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      manifestctl.sam.data.sfdc.net/swagger: disable
    name: watchdog-spark-operator-rolebinding
    namespace: flowsnake-watchdog
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: flowsnake-client-flowsnake-watchdog-Role
  subjects:
  - kind: ServiceAccount
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: v1
  data:
    analysis.py: |
      #!/usr/bin/env python
      # coding: utf-8

      """
      Runs the Spark Operator Watchdog script (e.g. watchdog-spark-on-k8s.sh). Performs analysis on the script's output to
      to classify failed runs by type of failure. Output and error code are then returned (with minimal additional decoration)
      to the caller (i.e. to the cliChecker Watchdog Go code). Additionally computes timing metrics (e.g. interval between
      Spark Driver requesting an exectuor and executor pod running). These timing metrics are reported for successful runs
      as well.

      The purpose of this program is to make it easier to determine which types of failures are the primary causes of
      script failures (and thus Flowsnake Service availability gaps). Determining what went wrong with a failed run requires
      carefully looking through the log, which was previously a tedious and manual process.

      Example: running analysis on test data:
      $ flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/watchdog-spark-on-k8s-analysis.py --test-dir flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/tests
      ✓ timeout_executor_slow_pod_creation_001.txt: TIMEOUT_EXECUTOR_SLOW_POD_CREATION
      ✓ driver_init_error_001.txt: DRIVER_INIT_ERROR
      ✓ etcd_no_leader_001.txt: {"class": "TIMEOUT_EXECUTOR_SLOW_POD_CREATION", "exception": "KubernetesClientException", "exception_cause": "ETCD_NO_LEADER"}
      ✓ exec_allocator_did_not_run_002.txt: EXECUTOR_ALLOCATOR_DID_NOT_RUN
      ✓ etcd_no_leader_002.txt: {"class": "SPARK_SUBMIT_FAILED", "exception": "KubernetesClientException", "exception_cause": "ETCD_NO_LEADER"}
      ✓ exec_allocator_did_not_run_001.txt: EXECUTOR_ALLOCATOR_DID_NOT_RUN
      ✓ timeout_executor_slow_pod_creation_002.txt: TIMEOUT_EXECUTOR_SLOW_POD_CREATION
      ✓ timeout_executor_slow_pod_creation_003.txt: TIMEOUT_EXECUTOR_SLOW_POD_CREATION
      ✓ timeout_exec_allocator_late_001.txt: TIMEOUT_EXECUTOR_ALLOCATOR_LATE
      ✓ scheduler_assume_pod_001.txt: SCHEDULER_ASSUME_POD

      Example: as above, but also writing metrics to Funnel:
      $ flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/watchdog-spark-on-k8s-analysis.py --test-dir flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/tests --metrics --sfdchosts /sfdchosts/hosts.json --watchdog-config /config/watchdog.json --host fs1shared0-flowsnakemastertest1-3-prd.eng.sfdc.net

      Example: as above, but writing to Funnel using defaults appropriate for local development:
      $ flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/watchdog-spark-on-k8s-analysis.py --test-dir flowsnake/templates/watchdog/spark-on-k8s-canary-scripts/tests --metrics --dev

      Metrics written with the --dev flag can be found using Argus expressions
      GROUPBYTAG(-15m:sam.watchdog.CORP.NONE.flowsnake-local-test:cliChecker.SparkOperatorTest.FailureAnalysis:none, #class#, #exception#, #exception_cause#, #SUM#)
      or, to also group by app:
      GROUPBYTAG(-15m:sam.watchdog.CORP.NONE.flowsnake-local-test:cliChecker.SparkOperatorTest.FailureAnalysis:none, #app#, #class#, #exception#, #exception_cause#, #SUM#)
      and for timing:
      GROUPBYTAG(-15m:sam.watchdog.CORP.NONE.flowsnake-local-test:cliChecker.SparkOperatorTest.Times.*:none, #app#, #succeeded#, #AVERAGE#)

      The GROUPBYTAG facilitates display of optional tags per
      https://gus.lightning.force.com/lightning/r/0D5B000000sQcBnKAK/view

      Real results from live fleets can be found using Argus expressions
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none, #class#, #exception#, #exception_cause#, #SUM#)
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none, #app#, #class#, #exception#, #exception_cause#, #SUM#)
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*:none, #app#, #succeeded#, #AVERAGE#)

      Or to separate out estates and/or data centers, include the #estate# and/or #dc# tag in the grouping:
      GROUPBYTAG(-15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none, #estate#, #class#, #exception#, #exception_cause#, #SUM#)

      To view all metric and tag permutations, omit grouping and aggregation:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.FailureAnalysis:none

      To view timing metrics:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*:avg
      Or, to separate out times per spark application:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*{app=*}:avg

      To view all metric and tag permutations, omit grouping and aggregation:
      -15m:sam.watchdog.*.NONE.*flowsnake*:cliChecker.SparkOperatorTest.Times.*:none

      TODO: Make a dashboard
      """

      from __future__ import print_function
      from argparse import ArgumentParser
      import calendar
      import os
      import re
      import subprocess
      import sys


      """
      Failure analysis metric. Recorded only when the result is a failure. Value always is 1. Metric tags indicate
      analysis of what went wrong.
      """
      FAILURE_ANALYSIS_METRIC_NAME = 'FailureAnalysis'
      #
      # Analysis dict keys
      # The result of the analysis is a string,string map, which is then sent to Argus as tags on a metric.
      CLASSIFICATION = 'class'  # Overall classification of the failure
      EXCEPTION = 'exception'  # Java class of most pertinent Exception, if any
      EXCEPTION_CAUSE = 'exception_cause'  # Determined cause of the Exception
      ANALYSIS_KEYS = [CLASSIFICATION, EXCEPTION, EXCEPTION_CAUSE]


      r_app_created = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - sparkapplication "(?P<app>.*)" created')
      r_driver_pod_creation_event = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver: (?P<state>.*) on host.*')
      r_driver_pod_creation_event_pending = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver: Pending on host.*')
      r_driver_pod_initializing = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver(:| changed to) (PodInitializing|Init)')
      r_driver_pod_running = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver(:| changed to) Running')
      r_driver_pod_completed = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-driver changed to Completed')
      r_spark_submit_failed = re.compile(r'failed to run spark-submit')
      r_driver_context_app_submitted = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*SparkContext.* - Submitted application')
      r_driver_context_jar_added = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*SparkContext.* - Added JAR file:')
      r_exec_allocator = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*ExecutorPodsAllocator.* - Going to request [0-9]* executors from Kubernetes')
      r_timeout = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Timeout reached. Aborting wait for SparkApplication .* even though in non-terminal state (?P<state>\w*)\.')
      #r_ driver_running_event = re.compile(r'SparkDriverRunning\s+([0-9]+)([sm])\s+spark-operator\s+Driver .* is running')
      r_exec_pod_creation_event = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+: (?P<state>.*) on host.*')
      r_exec_pod_creation_event_pending = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+: Pending on host.*')
      r_exec_pod_initializing = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+(:| changed to) (PodInitializing|Init)')
      r_exec_pod_running = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+ changed to Running.')
      r_exec_registered_time = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*KubernetesClusterSchedulerBackend.*Registered executor')
      r_driver_job_finished = re.compile(r'(?P<spark_time>[- :0-9]*) INFO.*DAGScheduler.*Job 0 finished')
      r_app_completed = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - SparkApplication .* has terminated .* State is COMPLETED')
      r_complete = re.compile(r'\[(?P<epoch>[0-9]+)\] .* - .*Completion of .* test')

      """
      Timing analysis metrics. Recorded whenever the data could be obtained, including for successful runs. Value is time in
      seconds. Each time has its own metric.
      """
      TIMING_METRIC_SUCCESS_TAG = 'succeeded'

      # Interval between script creating Spark Application and Spark Operator creating a driver pod. Includes Spark Operator time-to-react and Spark Operator time-to-spark-submit
      TIMING_METRIC_NAME_DRIVER_POD_DETECTED = 'AppCreationToDriverPodDetected'
      # Interval between pending driver pod and scheduled driver pod. Only present when the driver got stuck pending instead of being directly created.
      TIMING_METRIC_NAME_DRIVER_POD_PENDING_DELAY = 'DriverPodSchedulingDelay'
      # Interval between driver pod scheduled and driver pod Running. This is container creation, image pull, and MadKub execution
      TIMING_METRIC_NAME_DRIVER_POD_INITIALIZATION = 'DriverPodInitialization'
      # Interval between driver pod Running and driver submitting the Spark Context for the App. Primarily JVM bootup and Spark start time.
      TIMING_METRIC_NAME_DRIVER_APP_SUBMIT = 'DriverPodAppSubmit'
      # Interval between driver pod submitting the Spark Context for the App and adding its JAR. (Believe that JAR added is logged after all prep work prior to requesting executors has been completed)
      TIMING_METRIC_NAME_DRIVER_APP_LOAD = 'DriverPodAppLoad'
      # Interval between driver pod completing job application load and requesting executors to perform the work.
      TIMING_METRIC_NAME_DRIVER_APP_STARTUP = 'DriverPodAppStart'
      # Interval between driver requesting executor(s) and that it registers with the driver to start doing work
      TIMING_METRIC_NAME_EXECUTOR_WAIT_TOTAL = 'ExecutorTotalWait'
      # Interval between driver requesting executor(s) and detecting the executor pod.
      TIMING_METRIC_NAME_EXEC_POD_DETECTED = 'ExecutorAllocatorToPodDetected'
      # Interval between pending executor pod and scheduled executor pod. Only present when the executor got stuck pending instead of being directly created.
      TIMING_METRIC_NAME_EXEC_POD_PENDING_DELAY = 'ExecutorPodSchedulingDelay'
      # Interval between executor pod scheduled and executor pod Running
      TIMING_METRIC_NAME_EXEC_POD_INITIALIZATION = 'ExecutorPodInitialization'
      # Interval between executor pod running and executor picking up work from the driver
      TIMING_METRIC_NAME_EXEC_REGISTRATION = 'ExecutorPodRegistration'
      # Interval between executor picking up work from the driver and job completion
      TIMING_METRIC_NAME_JOB_RUNTIME = 'JobRunTime'
      # Interval between Spark job completion driver pod state changing to Completed
      TIMING_METRIC_NAME_DRIVER_CLEANUP = 'DriverCleanup'
      # Interval between Spark driver pod being Completed and Spark App being Completed. This is Spark Operator queue processing time.
      # Note: if the Spark Operator is not backlogged, it might detect driver completion before the analysis script, leading to a negative time being reported here.
      TIMING_METRIC_NAME_SPARKAPP_CLEANUP = 'SparkAppCleanup'


      # interval name -> (start regex, end regex). This the blueprint of what is to be computed.
      time_regex = {
          TIMING_METRIC_NAME_DRIVER_POD_DETECTED: (r_app_created, r_driver_pod_creation_event),  # Average: 7s
          TIMING_METRIC_NAME_DRIVER_POD_PENDING_DELAY: (r_driver_pod_creation_event_pending, r_driver_pod_initializing),  # Average: 3s
          TIMING_METRIC_NAME_DRIVER_POD_INITIALIZATION: (r_driver_pod_initializing, r_driver_pod_running),  # Average: 10s
          TIMING_METRIC_NAME_DRIVER_APP_SUBMIT: (r_driver_pod_running, r_driver_context_app_submitted),  # Average: 20s
          TIMING_METRIC_NAME_DRIVER_APP_LOAD: (r_driver_context_app_submitted, r_driver_context_jar_added),  # Average: 35s
          TIMING_METRIC_NAME_DRIVER_APP_STARTUP: (r_driver_context_jar_added, r_exec_allocator),  # Average:43s
          TIMING_METRIC_NAME_EXECUTOR_WAIT_TOTAL: (r_exec_allocator, r_exec_registered_time),  # Average: 16s
          TIMING_METRIC_NAME_EXEC_POD_DETECTED: (r_exec_allocator, r_exec_pod_creation_event),  # Average: 3s
          TIMING_METRIC_NAME_EXEC_POD_PENDING_DELAY: (r_exec_pod_creation_event_pending, r_exec_pod_initializing),  # Average: 3s
          TIMING_METRIC_NAME_EXEC_POD_INITIALIZATION: (r_exec_pod_initializing, r_exec_pod_running),  # Average: 10s
          TIMING_METRIC_NAME_EXEC_REGISTRATION: (r_exec_pod_running, r_exec_registered_time),  # Average: 4s
          TIMING_METRIC_NAME_JOB_RUNTIME: (r_exec_registered_time, r_driver_job_finished),  # Average: 15s
          TIMING_METRIC_NAME_DRIVER_CLEANUP: (r_driver_job_finished, r_driver_pod_completed),  # Average: 40s
          TIMING_METRIC_NAME_SPARKAPP_CLEANUP: (r_driver_pod_completed, r_app_completed),  # Average: 4s
      }

      TAG_APP = 'app'  # Name of the Spark Application. Same across concurrent watchdog instances. Roughly represents feature being tested.
      TAG_APP_ID = 'app_id'  # Id of the Spark Application. Unique across concurrent executions but recurring over time.
      TAG_ESTATE = 'estate'  # Estate this metric was emitted from ("pod" of the Argus scope)
      TAG_DC = 'dc'  # Datacenter this metric was emitted from

      # ------------ Funnel client code adapted from
      # https://git.soma.salesforce.com/monitoring/collectd-write_funnel_py/blob/18ef838f5a6221450e51ee2d7beb984adb0a3dc7/funnel_writer.py
      # ------------
      import httplib
      import time
      import json
      from os import R_OK, access
      from os.path import isfile, exists
      import collections
      import logging
      import socket

      MAX_TRIES = 10
      THIS_SCRIPT = os.path.basename(__file__)

      logging.basicConfig(
          level=logging.INFO,
          format='[%(asctime)s] %(filename)s:%(lineno)d %(levelname)5s - %(message)s'
      )

      Metric = collections.namedtuple(
          'Metric', ('service', 'name', 'value', 'timestamp', 'context', 'tags'))

      MetricContext = collections.namedtuple(
          'MetricContext', ('datacenter', 'superpod', 'pod', 'host'))


      class MetricEncoder(json.JSONEncoder):
          def encode(self, obj):
              if isinstance(obj, (list, tuple)):
                  batched_list = []
                  for o in obj:
                      batched_list.append(self._translate(o))
                  result = json.JSONEncoder.encode(self, batched_list)
              else:
                  result = json.JSONEncoder.encode(self, self._translate(obj))
              logging.debug('Encoded metric(s): %s' % result)
              return result

          @staticmethod
          def _translate(metric):
              assert isinstance(metric, Metric)
              all_tags = {}
              all_tags.update(metric.context._asdict())
              all_tags.update(metric.tags)
              metric_content = {
                  'service': metric.service,
                  'metricName': metric.name,
                  'metricValue': metric.value,
                  'timestamp': metric.timestamp,
                  'tags': all_tags,
              }
              return metric_content


      class FunnelException(Exception):
          pass


      class FunnelClient():
          def __init__(self, funnel_endpoint,
                       metric_scheme_fingerprint="AVG7NnlcHNdk4t_zn2JBnQ",
                       timeout_seconds=10,
                       funnel_debug=False,
                       cert_path=None,
                       key_path=None,
                       http_allowed=False,
                       ):
              self.funnel_endpoint = funnel_endpoint
              self.fingerprint = metric_scheme_fingerprint
              self.timeout = timeout_seconds
              self.debug = str(funnel_debug).lower()
              self.certpath = cert_path
              self.keypath = key_path
              self.https_allowed = http_allowed

              self.url = '/funnel/v1/publishBatch?avroSchemaFingerprint=%s&debug=%s' % (self.fingerprint, self.debug)
              self.has_certs = self.certpath and exists(self.certpath) and isfile(self.certpath) and \
                               access(self.certpath, R_OK) and \
                               self.keypath and exists(self.keypath) and isfile(self.keypath) and access(self.keypath, R_OK)

          def post_request(self, post_data, numberofmetrics, tries):
              try:
                  # We rely on the fact that in idb if the funnel-server doesn't have a port
                  # then it's using HTTPS, else HTTP
                  if ":" not in self.funnel_endpoint and self.has_certs and self.https_allowed:
                      connection = httplib.HTTPSConnection(self.funnel_endpoint,
                                                           key_file=self.keypath, cert_file=self.certpath,
                                                           timeout=self.timeout)
                  else:
                      connection = httplib.HTTPConnection(self.funnel_endpoint, timeout=self.timeout)

                  headers = {"Content-type": "application/json"}

                  connection.request('POST', self.url, post_data, headers)
                  response = connection.getresponse()
                  response_body = response.read()
                  logging.debug("POST %s -> %s (metricssize:%d merticsnum:%d tries:%d)" % (
                      self.funnel_endpoint, response_body, sys.getsizeof(post_data), numberofmetrics, tries))

                  if response.status != 200:
                      return False, response_body
                  return True, None
              except Exception as e:
                  return False, str(e)

          def publish_batch(self, metrics):
              sdata = json.dumps(metrics, cls=MetricEncoder)
              message = ''
              for retry in range(0, MAX_TRIES):
                  success, message = self.post_request(sdata, len(metrics), retry+1)
                  if success:
                      return True
              raise FunnelException(message)
      # --------- End adapted metrics code


      parser = ArgumentParser()
      mode_group = parser.add_mutually_exclusive_group(required=True)
      mode_group.add_argument("--command", action='store_true',
                          help="Spark Operator Watchdog script (and arguments) to execute")
      mode_group.add_argument("--analyze", dest="analyze",
                          help="Process the provided static content rather than executing a script")
      mode_group.add_argument("--test-dir", dest="test_dir",
                              help="Process all files in the provided directory as static content. First line of each file must be asserted result.")
      parser.add_argument("--sfdchosts",
                          help="Path of SAM sfdchosts file. Required for metrics generation")
      parser.add_argument("--watchdog-config",
                          help="Path of SAM Watchdog config file. Required for metrics generation")
      parser.add_argument("--hostname",
                          help="Override hostname to use when determining metrics configuration")
      parser.add_argument("--metrics", action='store_true',
                          help="If set, metrics will be written indicating the analysis result")
      parser.add_argument("--dev", action='store_true',
                          help="If set, metrics can be written without specifying --sfdchosts or --watchdog-config. Uses hard-coded PRD Funnel endpoint, dc:CORP, superpod:NONE, pod:flowsnake-local-test.")
      parser.add_argument("--estate",
                          help="Override estate (Argus pod) to use when determining metrics configuration. For use in combination with --dev")
      args, additional_args = parser.parse_known_args()

      simple_regex_tests = {
          # Driver pod's init container errors out. Cause TBD.
          'DRIVER_INIT_ERROR': re.compile(r'Pod change detected.*-driver changed to Init:Error'),
          # Scheduler bug in Kubernetes <= 1.9.7 that randomly prevents re-use of pod name. No longer expected because pod names are now unique.
          'SCHEDULER_ASSUME_POD': re.compile(r"FailedScheduling.*AssumePod failed: pod .* state wasn't initial but get assumed"),
          # No available nodes. Error could have been transient, but for now just assume if it was seen is caused the failure.
          'SCHEDULER_NO_NODES': re.compile(r"FailedScheduling.*0/[0-9]* nodes are available"),
          # This should be accompanied by a useful Exception
          'SPARK_CONTEXT_INIT_ERROR': re.compile(r'Error initializing SparkContext'),
          # This one might be due to IP exhaustion; need to check kubelet logs. https://salesforce.quip.com/i0ThASBMoHqf#VCTACATj2IO
          'DOCKER_SANDBOX': re.compile(r'Failed create pod sandbox'),
          'KUBECTL_MAX_TRIES_TIMEOUT': re.compile(r'Invocation \([0-9/]*\) of \[kubectl .*\] failed \(timed out \([0-9]*s\)\). Giving up.'),
          'DRIVER_EVICTED': re.compile(r'NodeControllerEviction.*node-controller.*Marking for deletion Pod .*-driver'),
          'MADKUB_INIT_EMPTY_DIR': re.compile(r'Error: failed to start container "madkub-init": .*kubernetes.io~empty-dir/datacerts'),
          'CONNECTION_REFUSED': re.compile(r'The connection to the server 10.254.208.1:443 was refused'),
          'NOT_LOGGED_IN_UNAUTHORIZED': re.compile(r'error: You must be logged in to the server \(Unauthorized\)'),
          'IMAGE_PULL': re.compile(r'Pod change detected: .* changed to ImagePullBackOff')
      }

      # These might match against runs that have better matches from simple_regex_tests; prefer the other result
      simple_regex_tests_tier2 = {
          # Something killed the driver; don't know what. Prefer DRIVER_EVICTED if there is evidence of eviction.
          # Treat observed kubelet event and JVM reporting SIGTERM equivalently
          'DRIVER_KILLED': re.compile(r'(Killing.*kubelet.*Killing container.*driver:Need to kill Pod|SparkApplicationFailed.*driver pod failed with ExitCode: 143)')

      }

      metrics_enabled = False
      if args.metrics:
          hostname = args.hostname if args.hostname else socket.gethostname()
          if args.dev:
              funnel_client = FunnelClient('ajna0-funnel1-0-prd.data.sfdc.net:80')
              estate = args.estate if args.estate else 'flowsnake-local-test'
              metric_context = MetricContext('CORP', 'NONE', estate, hostname)
              metrics_enabled = True
          elif not args.sfdchosts or not args.watchdog_config:
              logging.error("Cannot emit metrics: --sfdchosts and --watchdog-config are both required (or --dev)")
          else:
              if args.estate:
                  logging.error("Cannot specify estate except in combination with --dev")
              else:
                  try:
                      with open(args.sfdchosts) as f:
                          host_data = json.load(f)
                          try:
                              host_entry = next(e for e in host_data['hosts'] if e['hostname'] == hostname)
                              kingdom = host_entry['kingdom'].upper()
                              superpod = host_entry['superpod'].upper()
                              pod = host_entry['estate']
                          except StopIteration:
                              raise StandardError("Cannot emit metrics: host %s not found in sfdchosts" % hostname)
                      with open(args.watchdog_config) as f:
                          funnel_endpoint = json.load(f)['funnelEndpoint']
                      funnel_client = FunnelClient(funnel_endpoint)
                      metric_context = MetricContext(kingdom, superpod, pod, hostname)
                      metrics_enabled = True
                  except StandardError as e:
                      logging.exception("Cannot emit metrics: error parsing sfdchosts %s and watchdog-config %s",
                                        args.sfdchosts, args.watchdog_config)


      # Regex for fully-qualified Java class name https://stackoverflow.com/a/5205467/708883
      r_exception = re.compile(r'(?P<cause>(Caused by: )?)(?P<package>[a-zA-Z_$][a-zA-Z\d_$]*\.)*(?P<class>[a-zA-Z_$][a-zA-Z\d_$]*Exception): (?P<message>.*)')
      simple_regex_exception_messages = {
          # Driver pod's init container errors out. Cause TBD.
          'ETCD_NO_LEADER': re.compile(r'client: etcd member .* has no leader'),
          'BROKEN_PIPE': re.compile(r'Broken pipe'),
          'SPARK_ADMISSION_WEBHOOK': re.compile(r'failed calling admission webhook "webhook\.sparkoperator\.k8s\.io'),
          'REMOTE_CLOSED_CONNECTION': re.compile(r'Remote host closed connection'),
          'CONNECTION_RESET': re.compile(r'Connection reset'),
          'KUBEAPI_SHUTDOWN': re.compile(r'Apisever is shutting down'),
          'RBAC': re.compile(r'(?:role.rbac.authorization.k8s.io ".*" not found|forbidden: User ".*" cannot .* in the namespace)'),
      }

      app = None
      app_id = None

      def spark_log_time_to_epoch(spark_time):
          """
          Convert time format of Spark logs to unix epoch (seconds)
          :param spark_time: UTC formatted e.g. 2019-05-15 00:31:56
          :return: unix epoch in seconds
          """
          return calendar.timegm(time.strptime(spark_time, "%Y-%m-%d %H:%M:%S"))


      def compute_times(output, succeeded=False):
          """
          Calculates time intervals between events in provided output. Side effect: sets global app_id and app variables.
          :param output: Output from spark operator execution
          :param succeeded: Whether output represents a successful execution
          :return: (metric -> int (seconds) dictionary, regex -> epoch dictionary)
          """
          timings = {}  # interval name -> computed interval in seconds
          global app, app_id
          error_states = {'Terminating', 'Unknown', 'Error'}
          # The times are computed by using two regular expressions; one marks the start of the interval and
          # one marks the end of the interval.

          # regex -> (epoch, match). Time value found for each regex. Memoize because regexes are used multiple times.
          regex_results = {}
          for r1, r2 in time_regex.values():
              for r in [r1, r2]:
                  if r not in regex_results:
                      m = r.search(output)
                      if m:
                          # Not all log lines express time in the same format, so need multiple conversion rules
                          # to get to epoch. Presume regex group names are standardized.
                          match_groups = m.groupdict()
                          if 'epoch' in match_groups:
                              regex_results[r] = int(match_groups['epoch'])
                          elif 'spark_time' in match_groups:
                              regex_results[r] = spark_log_time_to_epoch(match_groups['spark_time'])
                          else:
                              log("Bug: regex {} is supposed to extract times but has no recognized group names. Matched {}.".format(
                                  r.pattern, m.group(0)))
                      else:
                          # Record explicit failure ot match so we don't try this regex again
                          regex_results[r] = None

          # compute intervals now that we have found all the times.
          for interval_name, (r_start, r_end) in time_regex.iteritems():
              epoch_start = regex_results.get(r_start)
              epoch_end = regex_results.get(r_end)
              if epoch_start and epoch_end:
                  timings[interval_name] = epoch_end - epoch_start

          # Identify app creation
          m_app_created = r_app_created.search(output)
          if m_app_created:
              app_id = m_app_created.group('app')
              if app_id:
                  app = '-'.join(app_id.split('-')[0:-1])  # Assume app-name-uniqueid format

          if metrics_enabled:
              emit_timing_metrics(timings, succeeded)
          return (timings, regex_results)


      def add_standard_tags(tags):
          if app_id:
              tags[TAG_APP_ID] = app_id
          if app:
              tags[TAG_APP] = app
          tags[TAG_ESTATE] = metric_context.pod
          tags[TAG_DC] = metric_context.datacenter
          return tags


      def emit_timing_metrics(times, succeeded):
          tags = add_standard_tags({
              TIMING_METRIC_SUCCESS_TAG: "OK" if succeeded else "FAIL"
          })
          m_list = [
              Metric('sam.watchdog', ['cliChecker', 'SparkOperatorTest', 'Times', metric], seconds, int(time.time()), metric_context, tags)
              for metric, seconds in times.iteritems()]
          try:
              funnel_client.publish_batch(m_list)
          except Exception as e:
              logging.exception('Failed to send %d metrics to funnel' % len(m_list))


      def analyze_helper(output, timings):
          """
          Classifies failure in provided output
          :param output: output from failed spark operator execution
          :param timings: metric -> int (seconds) dictionary. See compute_times
          :return: class (as string)
          """
          for code, regex in simple_regex_tests.iteritems():
              if regex.search(output):
                  return code
          for code, regex in simple_regex_tests_tier2.iteritems():
              if regex.search(output):
                  return code

          # Check for termination due to timeout.
          m_timeout = r_timeout.search(output)
          if m_timeout:
              # We have observed the Spark Application state failing to update even though e.g. the driver runs to
              # completion.
              m_completed = r_driver_pod_completed.search(output)
              if m_completed and int(m_completed.group('epoch')) < int(m_timeout.group('epoch')):
                  # Driver pod completed before the timeout was reached.
                  return 'TIMEOUT_SPARK_APP_NOT_COMPLETING'
              if m_timeout.group('state') in {'RUNNING', 'SUBMITTED'}:
                  # TODO: Look at collected metrics for a better sense of what normal values are.
                  # timing -> (max_seconds, classification_if_exceeded)
                  thresholds = {
                      TIMING_METRIC_NAME_DRIVER_POD_DETECTED: (60, 'TIMEOUT_DRIVER_SLOW_POD_CREATION'),
                      TIMING_METRIC_NAME_DRIVER_POD_PENDING_DELAY: (60, 'TIMEOUT_DRIVER_SLOW_POD_SCHEDULING'),
                      TIMING_METRIC_NAME_DRIVER_POD_INITIALIZATION: (60, 'TIMEOUT_DRIVER_SLOW_POD_INIT'),
                      TIMING_METRIC_NAME_DRIVER_APP_SUBMIT: (60, 'TIMEOUT_DRIVER_SLOW_APP_SUBMIT'),
                      TIMING_METRIC_NAME_DRIVER_APP_LOAD: (60, 'TIMEOUT_DRIVER_SLOW_APP_LOAD'),
                      TIMING_METRIC_NAME_DRIVER_APP_STARTUP: (60, 'TIMEOUT_DRIVER_SLOW_APP_STARTUP'),
                      TIMING_METRIC_NAME_EXEC_POD_DETECTED: (30, 'TIMEOUT_EXECUTOR_SLOW_POD_CREATION'),
                      TIMING_METRIC_NAME_EXEC_POD_PENDING_DELAY: (60, 'TIMEOUT_EXECUTOR_SLOW_POD_SCHEDULING'),
                      TIMING_METRIC_NAME_EXEC_POD_INITIALIZATION: (30, 'TIMEOUT_EXECUTOR_SLOW_POD_INIT'),
                      TIMING_METRIC_NAME_EXEC_REGISTRATION: (30, 'TIMEOUT_EXECUTOR_SLOW_REGISTRATION'),
                      TIMING_METRIC_NAME_JOB_RUNTIME: (30, 'TIMEOUT_SPARK_SLOW_JOB_RUN'),
                      TIMING_METRIC_NAME_DRIVER_CLEANUP: (30, 'TIMEOUT_DRIVER_SLOW_CLEANUP'),
                  }
                  for metric, seconds in timings.iteritems():
                      if metric in thresholds:
                          threshold, classification = thresholds[metric]
                          if seconds >= threshold:
                              return classification
              return 'TIMEOUT_' + (m_timeout.group('state') or 'WITHOUT_STATE')
          else:
              # Failure was *not* due to a timeout.
              if r_spark_submit_failed.search(output):
                  return "SPARK_SUBMIT_FAILED"  # Exception classification will provide reason
              else:
                  return "UNRECOGNIZED_NON_TIMEOUT"


      def detect_exceptions(output):
          """
          Identifies noteworthy exceptions in provided output
          :param output: output from failed spark operator execution
          :return: class (as string)
          """
          exception = {}
          match_iterator = r_exception.finditer(output)
          # heuristic: assume that the first Exception is the most interesting, but if it has logged "Caused by" lines, use
          # the final reported cause. E.g. BazException is selected from the following:
          # FooException: foo
          #    ...
          # Caused by BarException: bar
          #    ...
          # Caused by BazException: baz
          #    ...
          # ...
          # QuuxException: quux
          #    ...
          # Caused by FnarfException: fnarf
          #    ...
          exception_match = None
          for m in match_iterator:
              if not exception_match:
                  # First exception found is better than no exception
                  exception_match = m
              elif m.group('cause'):
                  # If this is a cause, prefer it over the previously found exception

                  # Exception (ha!) to the rule: sometimes the cause is less specific. Don't prefer it in that case. E.g.
                  # javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake
                  # Caused by: java.io.EOFException: SSL peer shut down incorrectly
                  # "Remote host closed connection" actually seems more useful.
                  key_is_more_specific_than = {
                      'SSLHandshakeException': 'EOFException',
                      'SocketTimeoutException': 'SocketException',
                  }
                  if key_is_more_specific_than.get(exception_match.group('class')) == m.group('class'):
                      continue
                  else:
                      exception_match = m
              else:
                  # Otherwise we're done; subsequent exception blocks are probably just cascading errors
                  # Exception (ha!) to the rule: sometimes the first exception logged has fewer details.
                  key_is_more_specific_than = {
                      'KubernetesClientException': 'ProtocolException',
                  }
                  if key_is_more_specific_than.get(m.group('class')) == exception_match.group('class'):
                      exception_match = m
                  else:
                      break

          if exception_match:
              # Record the exception itself
              exception[EXCEPTION] = exception_match.group('class')
              # Record additional exception classification based on the message
              for code, regex in simple_regex_exception_messages.iteritems():
                  if regex.search(exception_match.group('message')):
                      exception[EXCEPTION_CAUSE] = code
          return exception


      def analyze(output, timings):
          """
          Classifies failure in provided output, identifies noteworthy exceptions, and emits metrics (if enabled)
          :param output: output from failed spark operator execution
          :param timings: metric -> int (seconds) dictionary. See compute_times
          :return: class (as string)
          """
          analysis = {
              CLASSIFICATION: analyze_helper(output, timings)
          }
          analysis.update(detect_exceptions(output))
          if metrics_enabled:
              emit_failure_analysis_metrics(analysis)
          return analysis


      def emit_failure_analysis_metrics(analysis):
          # Although the GROUPBY magic as described in the example metric queries can work around optional tags, it seems
          # expedient to just always populate the tags to prevent Argus surprises in the future.
          tags = add_standard_tags(dict([(key, analysis.get(key, "None")) for key in ANALYSIS_KEYS]))
          m_list = [
              Metric('sam.watchdog', ['cliChecker', 'SparkOperatorTest', FAILURE_ANALYSIS_METRIC_NAME], 1, int(time.time()), metric_context, tags)
          ]
          try:
              funnel_client.publish_batch(m_list)
          except Exception as e:
              logging.exception('Failed to send %d metrics to funnel' % len(m_list))


      def pretty_result(analysis):
          # Convert result to format used in the test files
          # Use bare classification if there is no additional info. Otherwise string representation of named tuple.
          return json.dumps(analysis, sort_keys=True) if len(analysis) > 1 else analysis[CLASSIFICATION]


      def log(s):
          print('+++ [{}] {}'.format(THIS_SCRIPT, s))


      if args.command:
          start = time.time()
          try:
              log("Executing and analyzing output of: {}".format(" ".join(additional_args)))
              output = subprocess.check_output(additional_args, stderr=subprocess.STDOUT)
              print(output, end='')
              timings, epochs = compute_times(output, succeeded=True)
              log("Times: {}".format(json.dumps(timings, sort_keys=True)))
              log("No errors ({}s)".format(int(time.time() - start)))
              sys.exit(0)
          except subprocess.CalledProcessError as e:
              print(e.output, end='')
              timings, epochs = compute_times(e.output)
              log("Analysis of failure [{}] ({}s): {}".format(
                  e.returncode,
                  int(time.time() - start),
                  pretty_result(analyze(e.output, timings))))
              log("Times: {}".format(json.dumps(timings, sort_keys=True)))
              sys.exit(e.returncode)
      elif args.analyze:
          with open(args.analyze, 'r') as file:
              output = file.read()
              timings, epochs = compute_times(output)
              print(pretty_result(analyze(output, timings)))
              print("Times: {}".format(json.dumps(timings, sort_keys=True)))
      elif args.test_dir:
          success = True
          for filename in sorted(os.listdir(args.test_dir)):
              with open(os.path.join(args.test_dir, filename), 'r') as file:
                  expect, output = file.read().split('\n', 1)
                  timings, epochs = compute_times(output)
                  text_result = pretty_result(analyze(output, timings))
                  if text_result == expect:
                      print(u"\u2713 {}: {}".format(filename, expect).encode('utf-8'))
                  else:
                      print(u"\u2718 {}: {} expected, {} obtained".format(filename, expect, text_result).encode('utf-8'))
                      success = False
                  # Too much visual noise. But occasionally useful during development.
                  # print("Times: {}".format(json.dumps(timings, sort_keys=True)))
          if not success:
              sys.exit(1)
      else:
          log("Bug: argparse failed to set a known operation mode.")
          sys.exit(1)
    check-impersonation.sh: |
      #!/usr/bin/bash

      # This test actually does not involve Spark applications at all, but it is part of verifying the Flowsnake v2 offering.
      # This test performs a minimal interaction with the Kubernetes API to verify connectivity, authentication, and
      # authorization.

      KUBECONFIG="$1"

      # Success of this command demonstrates successful connection via impersonation proxy and mapping to
      # user account flowsnake_test.flowsnake-watchdog (which in turn is bound to flowsnake-client-flowsnake-watchdog-Role)
      # (Success does not depend on whether there exist any sparkapplication resources in the namespace)
      kubectl -n ${2:-flowsnake-watchdog} get sparkapplications
    check-spark-operator.sh: "#!/usr/bin/bash\nset -o nounset\nset -o errexit\nset
      -o pipefail\n\n# Disable use of SAM's custom kubeconfig, restore default Kubernetes
      behavior (this cluster's kubeapi using service account token)\nunset KUBECONFIG\n\nNAMESPACE=flowsnake-watchdog\nKUBECTL_TIMEOUT_SECS=10\n#
      Give kubeapi 1 minute to recover. 10 second timeout, 7th request begins 60s
      after 1st.\nKUBECTL_ATTEMPTS=7\n\n# default test timeout minutes\nTIMEOUT_MINS=5\n\n#
      Parse command line arguments. https://stackoverflow.com/a/14203146\nPOSITIONAL=()\nwhile
      [[ $# -gt 0 ]]\ndo\nkey=\"$1\"\n\ncase $key in\n    --kubeconfig)\n    # Use
      a custom kubeconfig (e.g. to access via MadDog PKI certs and Impersonation Proxy)\n
      \   export KUBECONFIG=\"$2\"\n    shift # past argument\n    shift # past value\n
      \   ;;\n    --namespace)\n    # Use a custom namespace (default is flowsnake-watchdog)\n
      \   export NAMESPACE=\"$2\"\n    shift # past argument\n    shift # past value\n
      \   ;;\n    --kubectl-timeout)\n    # Specify timeout (seconds) for individual
      kubectl invocations (default is 5)\n    export KUBECTL_TIMEOUT_SECS=\"$2\"\n
      \   shift # past argument\n    shift # past value\n    ;;\n    --kubectl-attempts)\n
      \   # Specify number of attempts for individual kubectl invocations (default
      is 3)\n    export KUBECTL_ATTEMPTS=\"$2\"\n    shift # past argument\n    shift
      # past value\n    ;;\n    --timeout-mins)\n    # Specify a longer timeout for
      long running integration test (default is 5)\n    export TIMEOUT_MINS=\"$2\"\n
      \   shift # past argument\n    shift # past value\n    ;;\n    *)    # unknown
      option\n    POSITIONAL+=(\"$1\") # save it in an array for later\n    shift
      # past argument\n    ;;\nesac\ndone\nset -- \"${POSITIONAL[@]}\" # restore positional
      parameters\n\nTEST_RUNNER_ID=${TEST_RUNNER_ID:-$(cut -c1-8 < /proc/sys/kernel/random/uuid)}\n\n#
      Check if spec is a jsonnet template\nif [[ \".jsonnet\" == \"${1: -8}\" ]] ;
      then\n    SPEC_INPUT=$(basename \"$1\")\n    # Replace .jsonnet with -<ID>.json
      to get output filename\n    SPEC_PATH=/strata-test-specs-out/${SPEC_INPUT%%.*}-${TEST_RUNNER_ID}.json\n
      \   jsonnet -V imageRegistry=${DOCKER_REGISTRY} -V jenkinsId=${TEST_RUNNER_ID}
      -V dockerTag=${DOCKER_TAG} -V s3ProxyHost=${S3_PROXY_HOST} -V driverServiceAccount=${DRIVER_SERVICE_ACCOUNT}
      -V kingdom=${KINGDOM} -V estate=${ESTATE} ${1} | \\\n    python -c 'import json,sys;
      j=json.load(sys.stdin); j_clean=j if len(j.keys())>1 else j[j.keys()[0]]; print
      json.dumps(j_clean, indent=4)' > ${SPEC_PATH}\n    if [ -f \"${SPEC_PATH}\"
      ]; then\n        SPEC=\"${SPEC_PATH}\"\n    else\n        echo \"spec ${SPEC_PATH}
      doesn't exist.\"\n        exit 1\n    fi\nelse\n    # regular spec\n    SPEC=$1\nfi\n\nAPP_NAME=$(python
      -c 'import json,sys; print json.load(sys.stdin)[\"metadata\"][\"name\"]' < $SPEC)\nSELECTOR=\"sparkoperator.k8s.io/app-name=$APP_NAME\"\n#
      Exit after 5 minutes to ensure we exit before cliChecker kills us (10 mins)
      so that all output can be logged.\nTIMEOUT_SECS=$((60*$TIMEOUT_MINS))\n\n# output
      Unix time to stdout\nepoch() {\n    date '+%s'\n}\nSTART_TIME=$(epoch)\n\n#
      Format string for log output by decorating with date, time, app name\nformat()
      {\n    sed -e \"s/^/$(date +'%m%d %H:%M:%S') [$(epoch)] $APP_NAME - /\"\n}\n\n#
      Format and output provided string to stdout\nlog() {\n    if [[ \"$@\" != \"\"
      ]]; then\n        echo \"${@}\" | format\n    fi\n}\n\n# Format (with heading
      marker) and output provided string to stdout\nlog_heading() {\n    log \"========
      $@ ========\"\n}\n\n# Format (with sub-heading marker) and output provided string
      to stdout\nlog_sub_heading() {\n    log \"---- $@ ----\"\n}\n\n# Run kubectl
      in namespace.\n# Use for extracting programatic values; otherwise prefer kcfw_log
      for formatted output.\n#\n# stdout is printed without change.\n# stderr is log-formatted
      and printed.\n#\n# Operations are timed out after KUBECTL_TIMEOUT_SECS and retried
      KUBECTL_ATTEMPTS times upon timeout or non-zero exit\n# Timeout and retry events
      are printed to stderr\nkcfw() {\n    ATTEMPT=1\n    while true; do\n        #
      In addition to the timeout for this specific kubectl command, we need to check
      that the script hasn't\n        # passed its overall timeout.\n        EPOCH=$(epoch)\n
      \       stdout=$(mktemp /tmp/$(basename $0)-stdout.XXXXXX)\n        stderr=$(mktemp
      /tmp/$(basename $0)-stderr.XXXXXX)\n        # Capture result code, don't trigger
      errexit. https://stackoverflow.com/a/15844901\n        timeout --signal=9 ${KUBECTL_TIMEOUT_SECS}
      kubectl -n ${NAMESPACE} \"$@\" 2>${stderr} >${stdout} && RESULT=$? || RESULT=$?\n
      \       # Hack to simplify scripting: if you try to delete something and get
      back a NotFound, treat that as a success.\n        if [[ $(echo \"$@\" | grep
      -P '\\bdelete\\b') && $(grep -P '\\(NotFound\\).* not found' ${stderr}) ]];
      then\n            return 0\n        fi\n        # Hack to simplify scripting:
      'No resources found' is never useful.\n        # Goofy: get with a selector
      says \"No resources found.\" on stderr but delete says \"No resources found\"
      on stdout.\n        sed -i '/^No resources found\\.\\?$/d' ${stdout}\n        sed
      -i '/^No resources found\\.\\?$/d' ${stderr}\n        # Format captured stderr
      for logging and output it to stderr\n        cat ${stderr} | format >&2\n        rm
      ${stderr}\n        cat ${stdout}\n        rm ${stdout}\n        if [[ $RESULT
      == 0 ]]; then\n            # Success! We're done.\n            return $RESULT;\n
      \       fi;\n        MSG=\"Invocation ($ATTEMPT/$KUBECTL_ATTEMPTS) of [kubectl
      -n ${NAMESPACE} $@] failed ($(if (( $RESULT == 124 || $RESULT == 137 )); then
      echo \"timed out (${KUBECTL_TIMEOUT_SECS}s)\"; else echo $RESULT; fi)).\"\n
      \       if (( EPOCH - START_TIME >= TIMEOUT_SECS )); then\n            log \"$MSG
      Out of time. Giving up.\" >&2\n            return ${RESULT}\n        elif ((
      $ATTEMPT < $KUBECTL_ATTEMPTS )); then\n            log \"$MSG Will sleep $KUBECTL_TIMEOUT_SECS
      seconds and then try again.\" >&2\n            sleep ${KUBECTL_TIMEOUT_SECS}\n
      \       else\n            log \"$MSG Giving up.\" >&2\n            return ${RESULT}\n
      \       fi;\n        ATTEMPT=$(($ATTEMPT + 1))\n    done;\n}\n\n# Like kcfw,
      plus also apply log formatting to stdout.\nkcfw_log() {\n  # pipefail is set,
      so sed won't lose any failure exit code returned by kubectl\n  # stderr is already
      formatted by kcfw, so only need to add formatting to stdout\n  kcfw \"$@\" |
      format\n}\n\n# Extract the \"Events\" section from a kubectl description of
      a resource.\nevents() {\n    log_sub_heading \"Begin Events\"\n    # awk magic
      prints only the Name: line and the Events lines (terminated by a blank line).\n
      \   # Use kcfw and explicitly call format after so Awk can look for start-of-line.\n
      \   kcfw describe sparkapplication $APP_NAME | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)'
      | format\n    kcfw describe pod -l ${SELECTOR},spark-role=driver | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)'
      | format\n    kcfw describe pod -l ${SELECTOR},spark-role=executor | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)'
      | format\n    log_sub_heading \"End Events\"\n}\n\n# Return the state of the
      Spark application.\n# Terminal values are COMPLETED and FAILED https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#the-crd-controller\nstate()
      {\n    kcfw get sparkapplication $APP_NAME -o jsonpath='{.status.applicationState.state}'\n}\n\n#
      Output logs for specified pod to stdout\n# Future: Alternatively, generate a
      Splunk link?\ndeclare -A POD_LOGS_COLLECTED # pod name -> \"true\" if logs for
      pod were already collected\npod_log() {\n    # Checking for key with set -o
      nounset active: https://stackoverflow.com/a/13221491\n    if [[ -z \"${POD_LOGS_COLLECTED[$1]+present}\"
      ]]; then\n        log_sub_heading \"Begin $1 madkub-init Log\"\n        # set
      KUBECTL_ATTEMPTS=1 so that in kcfw() we do not retry and wait for \n        #
      getting the containers and their logs because the containers can exit\n        #
      thus getting the container log files is a best effort\n        # we will restore
      KUBECTL_ATTEMPTS to its previous value afterwards\n        KUBECTL_ATTEMPTS_SAVE=$KUBECTL_ATTEMPTS\n
      \       KUBECTL_ATTEMPTS=1\n        # || true to avoid failing script if pod
      has gone away.\n        kcfw logs -c madkub-init $1 || true\n        log_sub_heading
      \"End $1 madkub-init Log\"\n        CONTAINER_NAME=\"\"\n        for CONTAINER_NAME
      in $(kcfw get $1 -o jsonpath='{.spec.containers[*].name}'); do\n            log_sub_heading
      \"Begin container $CONTAINER_NAME Log in pod $1\"\n            # || true to
      avoid failing script if pod has gone away.\n            kcfw logs -c $CONTAINER_NAME
      $1 || true\n            log_sub_heading \"End container $CONTAINER_NAME Log\"\n
      \       done;\n        KUBECTL_ATTEMPTS=$KUBECTL_ATTEMPTS_SAVE\n        POD_LOGS_COLLECTED[\"$1\"]=\"true\"\n
      \   fi\n}\n\n# Log changes to pods spawned for SparkApplication\ndeclare -A
      PREVIOUS_POD_REPORTS # pod name -> \"<pod_status> on host <nodeName>\"\nreport_pod_changes()
      {\n    unset POD_REPORTS\n    declare -A POD_REPORTS # pod name -> \"<pod_status>
      on host <nodeName>\"\n    # Fetch pod names and their status for this SparkApplication\n
      \   # Note that the status from kubectl get contains a more informative single-term
      status than is available in the JSON.\n    # The JSON contains the phase (Pending
      -> Running -> Completed), which does not mention Init, and the detailed\n    #
      conditions and containerStatuses lists, which are difficult to summarize.\n
      \   # Relevant pods for our spark application have label metadata.labels.spark-app-selector=$APP_ID\n
      \   # Reading command output line by line: https://unix.stackexchange.com/a/52027\n
      \   while read POD_REPORT; do\n        POD_NAME=$(echo $POD_REPORT | cut -d'
      ' -f1)\n        REPORT=$(echo $POD_REPORT | cut -d' ' -f1 --complement)\n        POD_REPORTS[\"$POD_NAME\"]=\"${REPORT}\"\n
      \   done < <(kcfw get pods -l${SELECTOR} --show-all -o wide --no-headers | awk
      '{print $1, $3, \"on host\", $7}')\n\n    # Note: Initially used process substitution
      (as in FOO=$(comm <(...) <(...) )here, but that left defunct grandchild\n    #
      processes behind. Switch to temp files instead.\n    # (Not totally clear on
      why. Something like: process substitution never collects the exit status of
      the invoked\n    # process. Thus when the Bash script exits, the process gets
      re-parented to the cliChecker. But the cliChecker\n    # does not collect it
      either, so zombie process entries pile up.)\n    PREVIOUS_POD_NAMES_FILE=$(mktemp
      /tmp/$(basename $0)-previous-pods.XXXXXX)\n    CURRENT_POD_NAMES_FILE=$(mktemp
      /tmp/$(basename $0)-current-pods.XXXXXX)\n    # Write out the names of the pods.
      ${!MY_MAP[@]} yields the keys of the associative array\n    echo ${!PREVIOUS_POD_REPORTS[@]}
      | xargs -n1 | sort > \"$PREVIOUS_POD_NAMES_FILE\"\n    echo ${!POD_REPORTS[@]}
      | xargs -n1 | sort > \"$CURRENT_POD_NAMES_FILE\"\n    # Compare pod names from
      before with ones present now.\n    # Bash array set operations: See https://unix.stackexchange.com/a/104848\n
      \   REMOVED_POD_NAMES=$(comm -23 \"$PREVIOUS_POD_NAMES_FILE\" \"$CURRENT_POD_NAMES_FILE\")\n
      \   NEW_POD_NAMES=$(comm -13 \"$PREVIOUS_POD_NAMES_FILE\" \"$CURRENT_POD_NAMES_FILE\")\n
      \   EXISTING_POD_NAMES=$(comm -12 \"$PREVIOUS_POD_NAMES_FILE\" \"$CURRENT_POD_NAMES_FILE\")\n
      \   rm \"$PREVIOUS_POD_NAMES_FILE\"\n    rm \"$CURRENT_POD_NAMES_FILE\"\n\n
      \   # Can't simply copy associative arrays in bash, so perform maintenance on
      PREVIOUS_POD_REPORTS as we go.\n    for POD_NAME in ${REMOVED_POD_NAMES}; do\n
      \       log \"Pod change detected: ${POD_NAME} removed.\"\n        unset PREVIOUS_POD_REPORTS[\"$POD_NAME\"]\n
      \   done\n    for POD_NAME in ${NEW_POD_NAMES}; do\n        log \"Pod change
      detected: ${POD_NAME}: ${POD_REPORTS[\"${POD_NAME}\"]}.\";\n        PREVIOUS_POD_REPORTS[\"$POD_NAME\"]=${POD_REPORTS[\"${POD_NAME}\"]}\n
      \   done;\n    for POD_NAME in ${EXISTING_POD_NAMES}; do\n        # The hostname
      won't change, so only report the pod status. ${VAR%% *} means delete everything
      after the first\n        # space. Thus \"<pod_status> on host <nodeName>\" becomes
      \"<pod_status>\"\n        # http://tldp.org/LDP/abs/html/string-manipulation.html\n
      \       # Except if the previous status was 'Pending on host <none>', in which
      case this is the first opportunity to log\n        # the host name.\n        OLD_REPORT=\"${PREVIOUS_POD_REPORTS[${POD_NAME}]}\"\n
      \       NEW_REPORT=\"${POD_REPORTS[${POD_NAME}]}\"\n        if [[ \"${OLD_REPORT}\"
      != \"${NEW_REPORT}\" ]]; then\n            if [[ ${OLD_REPORT} == *\"on host
      <none>\" ]] && [[ ${NEW_REPORT} != *\"on host <none>\" ]]; then\n                REPORT_DISPLAY=\"${NEW_REPORT}\"\n
      \           else\n                REPORT_DISPLAY=\"${NEW_REPORT%% *}\"\n            fi\n
      \           log \"Pod change detected: ${POD_NAME} changed to ${REPORT_DISPLAY}
      (previously ${OLD_REPORT%% *}).\"\n            PREVIOUS_POD_REPORTS[\"$POD_NAME\"]=\"${NEW_REPORT}\"\n
      \           if [[ \"${NEW_REPORT%% *}\" == \"Completed\" ]] || [[ \"${NEW_REPORT%%
      *}\" == \"Error\" ]] || [[ \"${NEW_REPORT%% *}\" == \"Terminating\" ]]; then\n
      \               # Grab the logs now rather than waiting until the end of the
      script; the pod might be deleted by then.\n                pod_log po/${POD_NAME}\n
      \           fi\n        fi\n    done;\n}\n\n\n# ------ Initialize ---------\nlog_heading
      \"Beginning $APP_NAME test\"\n# Sanity-check kubeapi connectivity\nkcfw_log
      cluster-info\n\n# ------ Clean up prior runs ---------\nlog \"Cleaning up SparkApplication/Pod
      older than 1 hours from prior runs.\"\n# https://stackoverflow.com/questions/48934491/kubernetes-how-to-delete-pods-based-on-age-creation-time\nAPPS=$(kcfw
      get sparkapplication -o go-template='{{range .items}}{{.metadata.name}} {{.metadata.creationTimestamp}}{{\"\\n\"}}{{end}}'
      | awk '$2 <= \"'$(date -d'now-1 hours' -Ins --utc | sed 's/+0000/Z/')'\" { print
      $1 }')\nfor APP in ${APPS}; do\n    PODSELECTOR=\"sparkoperator.k8s.io/app-name=${APP}\"\n
      \   kcfw_log delete sparkapplication ${APP}\n    kcfw_log delete pod -l ${PODSELECTOR}\ndone\n\n#
      ------ Run ---------\nlog \"Creating SparkApplication $APP_NAME\"\nkcfw_log
      create -f $SPEC\nSPARK_APP_START_TIME=$(epoch)\n\n# If we've gotten this far,
      we'd like to collect as much forensic data as possible\nset +o errexit\n\nLAST_LOGGED=$(epoch)\nlog
      \"Waiting for SparkApplication $APP_NAME to reach a terminal state.\"\nSTATE=$(state)\nwhile
      true; do\n    EPOCH=$(epoch)\n    if $(echo ${STATE} | grep -P '(COMPLETED|FAILED)'
      > /dev/null); then\n        log \"SparkApplication $APP_NAME has terminated
      after $(($EPOCH - $SPARK_APP_START_TIME)) seconds. State is $STATE.\"\n        break\n
      \   fi\n    # Use start time of script for timeout computation in order to still
      exit in timely fashion even if setup was slow\n    if (( EPOCH - START_TIME
      >= TIMEOUT_SECS )); then\n        log \"Timeout reached. Aborting wait for SparkApplication
      $APP_NAME even though in non-terminal state $STATE.\"\n        break\n    fi\n
      \   if (( EPOCH - LAST_LOGGED > 60 )); then\n        log \"...still waiting
      for terminal state (currently $STATE) after $((EPOCH-SPARK_APP_START_TIME))
      seconds.\";\n        events;\n        LAST_LOGGED=${EPOCH}\n    fi;\n    sleep
      1;\n    report_pod_changes\n    STATE=$(state)\ndone;\nEXIT_CODE=$(echo ${STATE}
      | grep COMPLETED > /dev/null; echo $?)\n\n# ------ Report Results ---------\nreport_pod_changes\nevents\n\nPOD_NAME=$(kcfw
      get pod -l ${SELECTOR},spark-role=driver -o name)\nif [[ -z ${POD_NAME} ]];
      then\n    log \"Cannot locate driver pod. Maybe it never started? No logs to
      display.\"\nelse\n    pod_log ${POD_NAME}\nfi\n\nlog -------- Executor Pods
      ----------\nEXECUTOR_PODS=$(kcfw get pod -l ${SELECTOR},spark-role=executor
      -o name)\nfor POD_NAME in ${EXECUTOR_PODS}; do\n    pod_log ${POD_NAME}\ndone;\n\nif
      $(echo ${STATE} | grep -P '(COMPLETED|FAILED)' > /dev/null); then\n    # Delete
      so that Kubernetes is in a cleaner state when the next test execution starts\n
      \   log \"Cleaning up stuff for completed or failed test.\"\n    kcfw_log delete
      sparkapplication ${APP_NAME}\n    kcfw_log delete pod -l ${SELECTOR}\nfi\n\nlog_heading
      \"Completion of $APP_NAME test, returning $EXIT_CODE\"\nexit ${EXIT_CODE}\n"
    kubeconfig-impersonation-proxy: |
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority: /certs/ca.pem
          server: https://kubernetes-api-flowsnake-syd.data.sfdc.net
        name: kubernetes
      contexts:
      - context:
          cluster: kubernetes
          user: kubernetes
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: kubernetes
        user:
          client-certificate: /certs/client/certificates/client.pem
          client-key: /certs/client/keys/client-key.pem
  kind: ConfigMap
  metadata:
    name: watchdog-spark-on-k8s-script-configmap
    namespace: flowsnake
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    labels:
      name: watchdog-spark-operator
    name: watchdog-spark-operator
    namespace: flowsnake
  spec:
    replicas: 5
    selector:
      matchLabels:
        app: watchdog-spark-operator
        apptype: monitoring
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          madkub.sam.sfdc.net/allcerts: '{"certreqs": [{"cert-type": "client", "kingdom":
            "syd", "name": "watchdogsparkoperator", "role": "flowsnake_test.flowsnake-watchdog"}]}'
        labels:
          app: watchdog-spark-operator
          apptype: monitoring
          flowsnakeOwner: dva-transform
          flowsnakeRole: WatchdogSparkOperator
      spec:
        containers:
        - command:
          - /sam/watchdog
          - -role=CLI
          - -emailFrequency=24h
          - -timeout=2s
          - -funnelEndpoint=ajna0-funnel1-0-syd.data.sfdc.net:80
          - --config=/config/watchdog.json
          - -cliCheckerCommandTarget=SparkOperatorTest
          - --hostsConfigFile=/sfdchosts/hosts.json
          - -watchdogFrequency=1m
          - -alertThreshold=1m
          - -cliCheckerTimeout=6m
          - -includeCommandOutput=true
          - -kingdom=syd
          - -estate=syd-flowsnake_prod
          env:
          - name: DOCKER_TAG
            value: "22"
          - name: S3_PROXY_HOST
            value: public0-proxy1-0-syd.data.sfdc.net
          - name: DRIVER_SERVICE_ACCOUNT
            value: spark-driver-flowsnake-watchdog
          - name: DOCKER_REGISTRY
            value: ops0-artifactrepo1-0-syd.data.sfdc.net
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: KINGDOM
            value: syd
          - name: ESTATE
            value: syd-flowsnake_prod
          image: ops0-artifactrepo1-0-syd.data.sfdc.net/dva/flowsnake-spark-on-k8s-integration-test-runner:22
          imagePullPolicy: IfNotPresent
          name: watchdog
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: "1"
              memory: 1Gi
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /sfdchosts
            name: sfdchosts
          - mountPath: /watchdog-spark-scripts
            name: watchdog-spark-scripts
          - mountPath: /certs
            name: datacerts
        - args:
          - /sam/madkub-client
          - --madkub-endpoint
          - https://10.254.208.254:32007
          - --maddog-endpoint
          - https://all.pkicontroller.pki.blank.syd.prod.non-estates.sfdcsd.net:8443
          - --maddog-server-ca
          - /etc/pki_service/ca/security-ca.pem
          - --madkub-server-ca
          - /etc/pki_service/ca/cacerts.pem
          - --token-folder
          - /tokens
          - --kingdom
          - syd
          - --superpod
          - None
          - --estate
          - syd-flowsnake_prod
          - --refresher
          - --run-init-for-refresher-mode
          - --cert-folders
          - watchdogsparkoperator:/certs
          - --ca-folder
          - /etc/pki_service/ca
          - --funnel-endpoint
          - http://ajna0-funnel1-0-syd.data.sfdc.net:80
          env:
          - name: MADKUB_NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MADKUB_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MADKUB_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ops0-artifactrepo1-0-syd.data.sfdc.net/tnrp/sam/madkub:1.0.0-0000084-9f4a6ca6
          name: sam-madkub-integration-refresher
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 7337
          volumeMounts:
          - mountPath: /certs
            name: datacerts
          - mountPath: /tokens
            name: tokens
          - mountPath: /etc/pki_service/ca
            name: certificate-authority
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/certificates
            name: client-certificate
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/keys
            name: client-key
            readOnly: true
        hostNetwork: false
        initContainers:
        - args:
          - /sam/madkub-client
          - --madkub-endpoint
          - https://10.254.208.254:32007
          - --maddog-endpoint
          - https://all.pkicontroller.pki.blank.syd.prod.non-estates.sfdcsd.net:8443
          - --maddog-server-ca
          - /etc/pki_service/ca/security-ca.pem
          - --madkub-server-ca
          - /etc/pki_service/ca/cacerts.pem
          - --token-folder
          - /tokens
          - --kingdom
          - syd
          - --superpod
          - None
          - --estate
          - syd-flowsnake_prod
          - --cert-folders
          - watchdogsparkoperator:/certs
          - --ca-folder
          - /etc/pki_service/ca
          - --funnel-endpoint
          - http://ajna0-funnel1-0-syd.data.sfdc.net:80
          env:
          - name: MADKUB_NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MADKUB_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MADKUB_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ops0-artifactrepo1-0-syd.data.sfdc.net/tnrp/sam/madkub:1.0.0-0000084-9f4a6ca6
          name: sam-madkub-integration-init
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 7337
          volumeMounts:
          - mountPath: /certs
            name: datacerts
          - mountPath: /tokens
            name: tokens
          - mountPath: /etc/pki_service/ca
            name: certificate-authority
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/certificates
            name: client-certificate
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/keys
            name: client-key
            readOnly: true
        restartPolicy: Always
        serviceAccount: watchdog-spark-operator-serviceaccount
        serviceAccountName: watchdog-spark-operator-serviceaccount
        volumes:
        - configMap:
            name: watchdog
          name: config
        - configMap:
            defaultMode: 493
            name: watchdog-spark-on-k8s-script-configmap
          name: watchdog-spark-scripts
        - configMap:
            name: sfdchosts
          name: sfdchosts
        - emptyDir:
            medium: Memory
          name: datacerts
        - emptyDir:
            medium: Memory
          name: tokens
        - hostPath:
            path: /etc/pki_service/ca
          name: certificate-authority
        - hostPath:
            path: /etc/pki_service/platform/platform-client/certificates
          name: client-certificate
        - hostPath:
            path: /etc/pki_service/platform/platform-client/keys
          name: client-key
kind: List
metadata: {}
