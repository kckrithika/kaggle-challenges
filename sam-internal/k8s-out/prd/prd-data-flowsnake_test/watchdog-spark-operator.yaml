apiVersion: v1
items:
- apiVersion: v1
  automountServiceAccountToken: true
  kind: ServiceAccount
  metadata:
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      manifestctl.sam.data.sfdc.net/swagger: disable
    name: watchdog-spark-operator-rolebinding
    namespace: flowsnake-watchdog
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: flowsnake-client-flowsnake-watchdog-Role
  subjects:
  - kind: ServiceAccount
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: v1
  data:
    check-spark-operator.sh: |
      #!/usr/bin/bash
      set -o nounset
      set -o errexit
      set -o pipefail

      # Disable use of SAM's custom kubeconfig
      unset KUBECONFIG

      kcfw() {
        kubectl -n flowsnake-watchdog "$@"
      }

      SPEC=$1
      APP_NAME=${SPEC%.*}
      SELECTOR="sparkoperator.k8s.io/app-name=$APP_NAME"

      echo "Cleaning up $APP_NAME resources from prior runs"
      kcfw delete sparkapplication $APP_NAME || true
      kcfw delete pod -l $SELECTOR || true
      # Wait for pods from prior runs to delete.
      while ! $(kcfw get pod -l $SELECTOR 2>&1 | grep "No resources" > /dev/null); do sleep 1; done;

      echo "Creating SparkApplication $APP_NAME"
      kcfw create -f /watchdog-spark-operator/$SPEC

      echo "Waiting for SparkApplication $APP_NAME to complete"
      # Terminal values are COMPLETED and FAILED https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#the-crd-controller
      while ! $(kcfw get sparkapplication $APP_NAME -o jsonpath='{.status.applicationState.state}' | grep -P '(COMPLETED|FAILED)' > /dev/null); do sleep 1; done;
      echo ---- Begin Spark Driver Log ----
      kcfw logs $(kcfw get pod -l $SELECTOR -o name)
      echo ---- End Spark Driver Log ----
      echo "Terminal SparkApplication $APP_NAME state is $(kcfw get sparkapplication $APP_NAME -o jsonpath='{.status.applicationState.state}')"
      # Test successful iff final state is COMPLETED. Use exit code from grep.
      kcfw get sparkapplication $APP_NAME -o jsonpath='{.status.applicationState.state}' | grep COMPLETED > /dev/null
    watchdog-spark-operator.json:
      apiVersion: sparkoperator.k8s.io/v1beta1
      kind: SparkApplication
      metadata:
        name: watchdog-spark-operator
        namespace: flowsnake-watchdog
      spec:
        deps:
          jars:
          - local:///sample-apps/sample-spark-operator/extra-jars/*
        driver:
          coreLimit: 200m
          cores: 0.1
          labels:
            version: 2.4.0
          memory: 512m
          serviceAccount: spark-driver-flowsnake-watchdog
        executor:
          cores: 1
          instances: 1
          labels:
            version: 2.4.0
          memory: 512m
        image: ops0-artifactrepo2-0-prd.data.sfdc.net/dva/flowsnake-sample-spark-operator:ops0-artifactrepo1-0-prd.data.sfdc.net/docker-sam/jinxing.wang/spark-on-k8s-sample-apps:1
        imagePullPolicy: Always
        mainApplicationFile: local:///sample-apps/sample-spark-operator/sample-spark-operator.jar
        mainClass: org.apache.spark.examples.SparkPi
        mode: cluster
        restartPolicy:
          type: Never
        sparkVersion: ""
        type: Scala
    watchdog-spark-s3.json:
      apiVersion: sparkoperator.k8s.io/v1beta1
      kind: SparkApplication
      metadata:
        name: watchdog-spark-s3
        namespace: flowsnake-watchdog
      spec:
        deps:
          jars:
          - local:///sample-apps/spark-s3-integration/extra-jars/*
        driver:
          coreLimit: 200m
          cores: 0.1
          envVars:
            AWS_REGION: us-west-2
            AWS_SSE_KEY: ea96b117-8eee-4314-b214-8a125eb5242e
            DATA_FILE: /sample-apps/spark-s3-integration/constitution_of_india.txt
            LOG_SPARK_EVENTS_IN_S3: "true"
            S3_BUCKET: moana-spark-history
            S3_PATH: spark-test
          labels:
            version: 2.4.0
          memory: 512m
          secrets:
          - name: aws
            path: /etc/flowsnake/secrets/aws
            secretType: Generic
          serviceAccount: spark-driver-flowsnake-watchdog
        executor:
          cores: 1
          instances: 1
          labels:
            version: 2.4.0
          memory: 512m
        image: ops0-artifactrepo2-0-prd.data.sfdc.net/dva/flowsnake-sample-spark-operator:ops0-artifactrepo1-0-prd.data.sfdc.net/docker-sam/jinxing.wang/spark-on-k8s-sample-apps:1
        imagePullPolicy: Always
        mainApplicationFile: local:///sample-apps/spark-s3-integration/sample-spark-operator.jar
        mainClass: com.salesforce.dva.transform.flowsnake.demo.SparkS3Demo
        mode: cluster
        restartPolicy:
          type: Never
        sparkVersion: ""
        type: Scala
  kind: ConfigMap
  metadata:
    name: watchdog-spark-operator-configmap
    namespace: flowsnake
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    labels:
      name: watchdog-spark-operator
    name: watchdog-spark-operator
    namespace: flowsnake
  spec:
    selector:
      matchLabels:
        app: watchdog-spark-operator
        apptype: monitoring
    template:
      metadata:
        labels:
          app: watchdog-spark-operator
          apptype: monitoring
          flowsnakeOwner: dva-transform
          flowsnakeRole: WatchdogSparkOperator
      spec:
        containers:
        - command:
          - /sam/watchdog
          - -role=CLI
          - -emailFrequency=72h
          - -timeout=2s
          - -funnelEndpoint=ajna0-funnel1-0-prd.data.sfdc.net:80
          - --config=/config/watchdog.json
          - -cliCheckerCommandTarget=SparkOperatorTest
          - --hostsConfigFile=/sfdchosts/hosts.json
          - -watchdogFrequency=15m
          - -alertThreshold=45m
          - -cliCheckerTimeout=15m
          image: ops0-artifactrepo2-0-prd.data.sfdc.net/docker-release-candidate/tnrp/sam/hypersam:sam-0002530-db32f9dc
          imagePullPolicy: IfNotPresent
          name: watchdog-canary
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: "1"
              memory: 500Mi
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /sfdchosts
            name: sfdchosts
          - mountPath: /watchdog-spark-operator
            name: watchdog-spark-operator
        hostNetwork: true
        restartPolicy: Always
        serviceAccount: watchdog-spark-operator-serviceaccount
        serviceAccountName: watchdog-spark-operator-serviceaccount
        volumes:
        - configMap:
            name: watchdog
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: spark-application.json
              path: spark-application.json
            - key: check-spark-operator.sh
              mode: 493
              path: check-spark-operator.sh
            name: watchdog-spark-operator-configmap
          name: watchdog-spark-operator
        - configMap:
            name: sfdchosts
          name: sfdchosts
kind: List
metadata: {}
