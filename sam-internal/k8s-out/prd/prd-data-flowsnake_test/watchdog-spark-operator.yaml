apiVersion: v1
items:
- apiVersion: v1
  automountServiceAccountToken: true
  kind: ServiceAccount
  metadata:
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    annotations:
      manifestctl.sam.data.sfdc.net/swagger: disable
    name: watchdog-spark-operator-rolebinding
    namespace: flowsnake-watchdog
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: flowsnake-client-flowsnake-watchdog-Role
  subjects:
  - kind: ServiceAccount
    name: watchdog-spark-operator-serviceaccount
    namespace: flowsnake
- apiVersion: v1
  data:
    analysis.py: |
      #!/usr/bin/env python

      """
      Runs the Spark Operator Watchdog script (e.g. watchdog-spark-on-k8s.sh). Performs analysis on the script's output to
      to classify failed runs by type of failure. Output and error code are then returned unmodified to the caller
      (cliChecker Watchdog Go code).

      The purpose of this program is to make it easier to determine which types of failures are the primary causes of
      script failures (and thus Flowsnake Service availability gaps). Determining what went wrong with a failed run requires
      carefully looking through the log, which was previously a tedious and manual process.

      Analysis result is written to stderr after the stderr output of the script itself. The analysis results can be
      tallied relatively easily with Splunk (TODO: query goes here). In the future consider reporting them to Funnel.
      """

      from argparse import ArgumentParser
      import os
      import re
      import subprocess
      import sys

      # ------------ metrics-related code adapted from
      # https://git.soma.salesforce.com/monitoring/collectd-write_funnel_py/blob/18ef838f5a6221450e51ee2d7beb984adb0a3dc7/funnel_writer.py
      # ------------
      import httplib
      import time
      import json
      from os import R_OK, access
      from os.path import isfile, exists
      import collections
      import logging
      import socket

      MAX_TRIES = 10

      logging.basicConfig(
          level=logging.INFO,
          format='[%(asctime)s] %(filename)s:%(lineno)d %(levelname)5s - %(message)s'
      )

      Metric = collections.namedtuple(
          'Metric', ('service', 'name', 'value', 'timestamp', 'context', 'tags'))

      MetricContext = collections.namedtuple(
          'MetricContext', ('datacenter', 'superpod', 'pod', 'host'))


      class MetricEncoder(json.JSONEncoder):
          def encode(self, obj):
              if isinstance(obj, (list, tuple)):
                  batched_list = []
                  for o in obj:
                      batched_list.append(self._translate(o))
                  result = json.JSONEncoder.encode(self, batched_list)
              else:
                  result = json.JSONEncoder.encode(self, self._translate(obj))
              logging.debug('Encoded metric(s): %s' % result)
              return result

          @staticmethod
          def _translate(metric):
              assert isinstance(metric, Metric)
              all_tags = {}
              all_tags.update(metric.context._asdict())
              all_tags.update(metric.tags)
              metric_content = {
                  'service': metric.service,
                  'metricName': metric.name,
                  'metricValue': metric.value,
                  'timestamp': metric.timestamp,
                  'tags': all_tags,
              }
              return metric_content


      class FunnelException(Exception):
          pass


      class FunnelClient():
          def __init__(self, funnel_endpoint,
                       metric_scheme_fingerprint="AVG7NnlcHNdk4t_zn2JBnQ",
                       timeout_seconds=10,
                       funnel_debug=False,
                       cert_path=None,
                       key_path=None,
                       http_allowed=False,
                       ):
              self.funnel_endpoint = funnel_endpoint
              self.fingerprint = metric_scheme_fingerprint
              self.timeout = timeout_seconds
              self.debug = str(funnel_debug).lower()
              self.certpath = cert_path
              self.keypath = key_path
              self.https_allowed = http_allowed

              self.url = '/funnel/v1/publishBatch?avroSchemaFingerprint=%s&debug=%s' % (self.fingerprint, self.debug)
              self.has_certs = self.certpath and exists(self.certpath) and isfile(self.certpath) and \
                               access(self.certpath, R_OK) and \
                               self.keypath and exists(self.keypath) and isfile(self.keypath) and access(self.keypath, R_OK)

          def post_request(self, post_data, numberofmetrics, tries):
              try:
                  # We rely on the fact that in idb if the funnel-server doesn't have a port
                  # then it's using HTTPS, else HTTP
                  if ":" not in self.funnel_endpoint and self.has_certs and self.https_allowed:
                      connection = httplib.HTTPSConnection(self.funnel_endpoint,
                                                           key_file=self.keypath, cert_file=self.certpath,
                                                           timeout=self.timeout)
                  else:
                      connection = httplib.HTTPConnection(self.funnel_endpoint, timeout=self.timeout)

                  headers = {"Content-type": "application/json"}

                  connection.request('POST', self.url, post_data, headers)
                  response = connection.getresponse()
                  response_body = response.read()
                  logging.debug("POST %s -> %s (metricssize:%d merticsnum:%d tries:%d)" % (
                      self.funnel_endpoint, response_body, sys.getsizeof(post_data), numberofmetrics, tries))

                  if response.status != 200:
                      return False, response_body
                  return True, None
              except Exception as e:
                  return False, str(e)

          def publish_batch(self, metrics):
              sdata = json.dumps(metrics, cls=MetricEncoder)
              message = ''
              for retry in range(0, MAX_TRIES):
                  success, message = self.post_request(sdata, len(metrics), retry+1)
                  if success:
                      return True
              raise FunnelException(message)
      # --------- End adapted metrics code


      parser = ArgumentParser()
      mode_group = parser.add_mutually_exclusive_group(required=True)
      mode_group.add_argument("--command", action='store_true',
                          help="Spark Operator Watchdog script (and arguments) to execute")
      mode_group.add_argument("--analyze", dest="analyze",
                          help="Process the provided static content rather than executing a script")
      mode_group.add_argument("--test-dir", dest="test_dir",
                              help="Process all files in the provided directory as static content. First line of each file must be asserted result.")
      parser.add_argument("--sfdchosts",
                          help="Path of SAM sfdchosts file. Required for metrics generation")
      parser.add_argument("--watchdog-config",
                          help="Path of SAM Watchdog config file. Required for metrics generation")
      parser.add_argument("--hostname",
                          help="Override hostname to use when determining metrics configuration")
      parser.add_argument("--metrics", action='store_true',
                          help="If set, metrics will be written indicating the analysis result")
      args, additional_args = parser.parse_known_args()

      simple_regex_tests = {
          # Driver pod's init container errors out. Cause TBD.
          'DRIVER_INIT_ERROR': re.compile('Pod change detected.*-driver changed to Init:Error'),
          # Scheduler bug in Kubernetes <= 1.9.7 that randomly prevents re-use of pod name. No longer expected because pod names are now unique.
          'SCHEDULER_ASSUME_POD': re.compile("FailedScheduling.*AssumePod failed: pod .* state wasn't initial but get assumed")
      }

      r_driver_context_submitted = re.compile(r'SparkContext.* - Submitted application')
      r_driver_context_jar_added = re.compile(r'SparkContext.* - Added JAR file:')
      r_executor_allocator_ran = re.compile(r'ExecutorPodsAllocator.* - Going to request [0-9]* executors from Kubernetes')
      r_timeout_running = re.compile(r'Timeout reached. Aborting wait for SparkApplication .* even though in non-terminal state RUNNING.')
      #r_ driver_running_event = re.compile(r'SparkDriverRunning\s+([0-9]+)([sm])\s+spark-operator\s+Driver .* is running')
      r_driver_running_epoch = re.compile(r'\[([0-9]+)\] .* - Pod change detected: .*-driver changed to Running.')
      r_exec_running_epoch = re.compile(r'\[([0-9]+)\] .* - Pod change detected: .*-exec-[0-9]+ changed to Running.')

      def analyze_helper(combined_output):
          for code, regex in simple_regex_tests.iteritems():
              if regex.search(combined_output):
                  return code

          # Check for termination due to timeout.
          if r_timeout_running.search(combined_output):

              # Check for failures after the driver is running
              m = r_driver_running_epoch.search(combined_output)
              if m:
                  # Check for failure cases that occur after the driver is runnning
                  driver_running_epoch = int(m.group(1))
                  # This block digs into driver logs
                  if r_driver_context_submitted.search(combined_output):
                      # Check for failure cases that occur after the application JAR has been loaded
                      if r_driver_context_jar_added.search(combined_output):
                          # Requesting executors is the next thing to do after adding the application JAR. Unknown why it sometimes doesn't happen.
                          if not r_executor_allocator_ran.search(combined_output):
                              return 'EXECUTOR_ALLOCATOR_DID_NOT_RUN'
                  # Figure out when the executor started

                  # If we can't figure out a specific reason, look into what part was slow.
                  m = r_exec_running_epoch.search(combined_output)
                  if m:
                      exec_running_epoch = int(m.group(1))
                      if exec_running_epoch - driver_running_epoch >= 180:
                          # We're likely to not complete in time if it takes this long for the driver to launch an executor.
                          # If this is a frequent occurrence, we should get more precise. Specifically, when did
                          # r_executor_allocator_ran happen? That would shed some light on whether the driver was slow to
                          # request the executor, or whether the executor was slow to start.
                          # (In the case of timeout_executor_late_001.txt, it took over three minutes from driver Running to
                          # executors requested.)
                          return "TIMEOUT_EXECUTOR_LATE"

                  # NOTE: do not put checks for error messages here. Error messages are more informative that timeouts, so if
                  # we have an error message, we should return it. Therefore put all error message checks above the timeout
                  # checks.
                  return 'UNRECOGNIZED_TIMEOUT_DRIVER_RUNNING'
              else:
                  return "UNRECOGNIZED_TIMEOUT_DRIVER_NOT_RUNNING"
          return "UNRECOGNIZED"


      def analyze(combined_output):
          result = analyze_helper(combined_output)
          if metrics_enabled:
              emit_metrics(result)
          return result


      def emit_metrics(result):
          funnel_client = FunnelClient(funnel_endpoint)
          metric_context = MetricContext(kingdom, superpod, pod, hostname)
          m_list = [
              Metric('sam.watchdog', ['cliChecker', 'SparkOperatorTest', 'FailureAnalysis'], 1, int(time.time()), metric_context, {'class': result})
          ]
          try:
              funnel_client.publish_batch(m_list)
          except Exception as e:
              logging.exception('Failed to send %d metrics to funnel' % len(m_list))

      metrics_enabled = False
      if args.metrics:
          if args.hostname:
              hostname = args.hostname
          else:
              hostname = socket.gethostname()
          if not args.sfdchosts or not args.watchdog_config:
              logging.error("Cannot emit metrics: --sfdchosts and --watchdog-config are both required")
          else:
              try:
                  with open(args.sfdchosts) as f:
                      host_data = json.load(f)
                      try:
                          host_entry = next(e for e in host_data['hosts'] if e['hostname'] == hostname)
                          kingdom = host_entry['kingdom'].upper()
                          superpod = host_entry['superpod'].upper()
                          pod = host_entry['estate']
                      except StopIteration:
                          raise StandardError("Cannot emit metrics: host %s not found in sfdchosts" % hostname)
                  with open(args.watchdog_config) as f:
                      funnel_endpoint = json.load(f)['funnelEndpoint']
                  metrics_enabled = True
              except StandardError as e:
                  logging.exception("Cannot emit metrics: error parsing sfdchosts %s and watchdog-config %s",
                                    args.sfdchosts, args.watchdog_config)


      if args.command:
          try:
              print subprocess.check_output(additional_args, stderr=subprocess.STDOUT)
              sys.exit(0)
          except subprocess.CalledProcessError as e:
              print e.output
              print "Analysis of failure: {}".format(analyze(e.output))
              sys.exit(e.returncode)
      elif args.analyze:
          with open(args.analyze, 'r') as file:
              result = analyze(file.read())
              print result
      elif args.test_dir:
          success = True
          for filename in os.listdir(args.test_dir):
              with open(os.path.join(args.test_dir, filename), 'r') as file:
                  data = file.read()
                  expect, contents = data.split('\n', 1)
                  result = analyze(contents)
                  if result == expect:
                      print (u"\u2713 {}: {}".format(filename, expect))
                  else:
                      print (u"\u2718 {}: {} expected, {} obtained".format(filename, expect, result))
                      success = True
          if not success:
              sys.exit(1)
      else:
          print "Bug: argparse failed to set a known operation mode."
          sys.exit(1)
    check-impersonation.sh: |
      #!/usr/bin/bash

      # This test actually does not involve Spark applications at all, but it is part of verifying the Flowsnake v2 offering.
      # This test performs a minimal interaction with the Kubernetes API to verify connectivity, authentication, and
      # authorization.

      KUBECONFIG="$1"

      # Success of this command demonstrates successful connection via impersonation proxy and mapping to
      # user account flowsnake_test.flowsnake-watchdog (which in turn is bound to flowsnake-client-flowsnake-watchdog-Role)
      # (Success does not depend on whether there exist any sparkapplication resources in the namespace)
      kubectl -n ${2:-flowsnake-watchdog} get sparkapplications
    check-spark-operator.sh: |
      #!/usr/bin/bash
      set -o nounset
      set -o errexit
      set -o pipefail

      # Disable use of SAM's custom kubeconfig, restore default Kubernetes behavior (this cluster's kubeapi using service account token)
      unset KUBECONFIG

      NAMESPACE=flowsnake-watchdog
      KUBECTL_TIMEOUT_SECS=10
      # Give kubeapi 1 minute to recover. 10 second timeout, 7th request begins 60s after 1st.
      KUBECTL_ATTEMPTS=7

      # Parse command line arguments. https://stackoverflow.com/a/14203146
      POSITIONAL=()
      while [[ $# -gt 0 ]]
      do
      key="$1"

      case $key in
          --kubeconfig)
          # Use a custom kubeconfig (e.g. to access via MadDog PKI certs and Impersonation Proxy)
          export KUBECONFIG="$2"
          shift # past argument
          shift # past value
          ;;
          --namespace)
          # Use a custom namespace (default is flowsnake-watchdog)
          export NAMESPACE="$2"
          shift # past argument
          shift # past value
          ;;
          --kubectl-timeout)
          # Specify timeout (seconds) for individual kubectl invocations (default is 5)
          export KUBECTL_TIMEOUT_SECS="$2"
          shift # past argument
          shift # past value
          ;;
          --kubectl-attempts)
          # Specify number of attempts for individual kubectl invocations (default is 3)
          export KUBECTL_ATTEMPTS="$2"
          shift # past argument
          shift # past value
          ;;
          *)    # unknown option
          POSITIONAL+=("$1") # save it in an array for later
          shift # past argument
          ;;
      esac
      done
      set -- "${POSITIONAL[@]}" # restore positional parameters

      TEST_RUNNER_ID=${TEST_RUNNER_ID:-$(cut -c1-8 < /proc/sys/kernel/random/uuid)}

      # Check if spec is a jsonnet template
      if [[ ".jsonnet" == "${1: -8}" ]] ; then
          SPEC_INPUT=$(basename "$1")
          # Replace .jsonnet with -<ID>.json to get output filename
          SPEC_PATH=/strata-test-specs-out/${SPEC_INPUT%%.*}-${TEST_RUNNER_ID}.json
          jsonnet -V imageRegistry=${DOCKER_REGISTRY} -V jenkinsId=${TEST_RUNNER_ID} -V dockerTag=${DOCKER_TAG} -V s3ProxyHost=${S3_PROXY_HOST} -V driverServiceAccount=${DRIVER_SERVICE_ACCOUNT} ${1} | \
          python -c 'import json,sys; j=json.load(sys.stdin); j_clean=j if len(j.keys())>1 else j[j.keys()[0]]; print json.dumps(j_clean, indent=4)' > ${SPEC_PATH}
          if [ -f "${SPEC_PATH}" ]; then
              SPEC="${SPEC_PATH}"
          else
              echo "spec ${SPEC_PATH} doesn't exist."
              exit 1
          fi
      else
          # regular spec
          SPEC=$1
      fi

      APP_NAME=$(python -c 'import json,sys; print json.load(sys.stdin)["metadata"]["name"]' < $SPEC)
      SELECTOR="sparkoperator.k8s.io/app-name=$APP_NAME"
      # Exit after 5 minutes to ensure we exit before cliChecker kills us (10 mins) so that all output can be logged.
      TIMEOUT_SECS=$((60*5))

      # output Unix time to stdout
      epoch() {
          date '+%s'
      }
      START_TIME=$(epoch)

      # Format string for log output by decorating with date, time, app name
      format() {
          sed -e "s/^/$(date +'%m%d %H:%M:%S') [$(epoch)] $APP_NAME - /"
      }

      # Format and output provided string to stdout
      log() {
          if [[ "$@" != "" ]]; then
              echo "${@}" | format
          fi
      }

      # Format (with heading marker) and output provided string to stdout
      log_heading() {
          log "======== $@ ========"
      }

      # Format (with sub-heading marker) and output provided string to stdout
      log_sub_heading() {
          log "---- $@ ----"
      }

      # Run kubectl in namespace.
      # Use for extracting programatic values; otherwise prefer kcfw_log for formatted output.
      #
      # stdout is printed without change.
      # stderr is log-formatted and printed.
      #
      # Operations are timed out after KUBECTL_TIMEOUT_SECS and retried KUBECTL_ATTEMPTS times upon timeout or non-zero exit
      # Timeout and retry events are printed to stderr
      kcfw() {
          ATTEMPT=1
          while true; do
              # In addition to the timeout for this specific kubectl command, we need to check that the script hasn't
              # passed its overall timeout.
              EPOCH=$(epoch)
              stdout=$(mktemp /tmp/$(basename $0)-stdout.XXXXXX)
              stderr=$(mktemp /tmp/$(basename $0)-stderr.XXXXXX)
              # Capture result code, don't trigger errexit. https://stackoverflow.com/a/15844901
              timeout --signal=9 ${KUBECTL_TIMEOUT_SECS} kubectl -n ${NAMESPACE} "$@" 2>${stderr} >${stdout} && RESULT=$? || RESULT=$?
              # Hack to simplify scripting: if you try to delete something and get back a NotFound, treat that as a success.
              if [[ $(echo "$@" | grep -P '\bdelete\b') && $(grep -P '\(NotFound\).* not found' ${stderr}) ]]; then
                  return 0
              fi
              # Hack to simplify scripting: 'No resources found' is never useful.
              # Goofy: get with a selector says "No resources found." on stderr but delete says "No resources found" on stdout.
              sed -i '/^No resources found\.\?$/d' ${stdout}
              sed -i '/^No resources found\.\?$/d' ${stderr}
              # Format captured stderr for logging and output it to stderr
              cat ${stderr} | format >&2
              rm ${stderr}
              cat ${stdout}
              rm ${stdout}
              if [[ $RESULT == 0 ]]; then
                  # Success! We're done.
                  return $RESULT;
              fi;
              MSG="Invocation ($ATTEMPT/$KUBECTL_ATTEMPTS) of [kubectl -n ${NAMESPACE} $@] failed ($(if (( $RESULT == 124 || $RESULT == 137 )); then echo "timed out (${KUBECTL_TIMEOUT_SECS}s)"; else echo $RESULT; fi))."
              if (( EPOCH - START_TIME >= TIMEOUT_SECS )); then
                  log "$MSG Out of time. Giving up." >&2
                  return ${RESULT}
              elif (( $ATTEMPT < $KUBECTL_ATTEMPTS )); then
                  log "$MSG Will sleep $KUBECTL_TIMEOUT_SECS seconds and then try again." >&2
                  sleep ${KUBECTL_TIMEOUT_SECS}
              else
                  log "$MSG Giving up." >&2
                  return ${RESULT}
              fi;
              ATTEMPT=$(($ATTEMPT + 1))
          done;
      }

      # Like kcfw, plus also apply log formatting to stdout.
      kcfw_log() {
        # pipefail is set, so sed won't lose any failure exit code returned by kubectl
        # stderr is already formatted by kcfw, so only need to add formatting to stdout
        kcfw "$@" | format
      }

      # Extract the "Events" section from a kubectl description of a resource.
      events() {
          log_sub_heading "Begin Events"
          # awk magic prints only the Name: line and the Events lines (terminated by a blank line).
          # Use kcfw and explicitly call format after so Awk can look for start-of-line.
          kcfw describe sparkapplication $APP_NAME | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)' | format
          kcfw describe pod -l ${SELECTOR},spark-role=driver | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)' | format
          kcfw describe pod -l ${SELECTOR},spark-role=executor | awk '/Events:/{flag=1}/^$/{flag=0}(flag||/^Name:/)' | format
          log_sub_heading "End Events"
      }

      # Return the state of the Spark application.
      # Terminal values are COMPLETED and FAILED https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#the-crd-controller
      state() {
          kcfw get sparkapplication $APP_NAME -o jsonpath='{.status.applicationState.state}'
      }

      # Output logs for specified pod to stdout
      # Future: Alternatively, generate a Splunk link?
      pod_log() {
          log_sub_heading "Begin $1 Log"
          # || true to avoid failing script if pod has gone away.
          kcfw logs $1 || true
          log_sub_heading "End $1 Log"
      }

      # Log changes to pods spawned for SparkApplication
      declare -A PREVIOUS_POD_REPORTS # pod name -> "<pod_status> on host <nodeName>"
      report_pod_changes() {
          unset POD_REPORTS
          declare -A POD_REPORTS # pod name -> "<pod_name> <pod_status> on host <nodeName>"
          # Fetch pod names and their status for this SparkApplication
          # Note that the status from kubectl get contains a more informative single-term status than is available in the JSON.
          # The JSON contains the phase (Pending -> Running -> Completed), which does not mention Init, and the detailed
          # conditions and containerStatuses lists, which are difficult to summarize.
          # Relevant pods for our spark application have label metadata.labels.spark-app-selector=$APP_ID
          # Reading command ouptut line by line: https://unix.stackexchange.com/a/52027
          while read POD_REPORT; do
              POD_NAME=$(echo $POD_REPORT | cut -d' ' -f1)
              REPORT=$(echo $POD_REPORT | cut -d' ' -f1 --complement)
              POD_REPORTS["$POD_NAME"]="${REPORT}"
          done < <(kcfw get pods -l${SELECTOR} --show-all -o wide --no-headers | awk '{print $1, $3, "on host", $7}')

          # Note: Initially used process substitution (as in FOO=$(comm <(...) <(...) )here, but that left defunct grandchild
          # processes behind. Switch to temp files instead.
          # (Not totally clear on why. Something like: process substitution never collects the exit status of the invoked
          # process. Thus when the Bash script exits, the process gets re-parented to the cliChecker. But the cliChecker
          # does not collect it either, so zombie process entries pile up.)
          PREVIOUS_POD_NAMES_FILE=$(mktemp /tmp/$(basename $0)-previous-pods.XXXXXX)
          CURRENT_POD_NAMES_FILE=$(mktemp /tmp/$(basename $0)-current-pods.XXXXXX)
          # Write out the names of the pods. ${!MY_MAP[@]} yields the keys of the associative array
          echo ${!PREVIOUS_POD_REPORTS[@]} | xargs -n1 | sort > "$PREVIOUS_POD_NAMES_FILE"
          echo ${!POD_REPORTS[@]} | xargs -n1 | sort > "$CURRENT_POD_NAMES_FILE"
          # Compare pod names from before with ones present now.
          # Bash array set operations: See https://unix.stackexchange.com/a/104848
          REMOVED_POD_NAMES=$(comm -23 "$PREVIOUS_POD_NAMES_FILE" "$CURRENT_POD_NAMES_FILE")
          NEW_POD_NAMES=$(comm -13 "$PREVIOUS_POD_NAMES_FILE" "$CURRENT_POD_NAMES_FILE")
          EXISTING_POD_NAMES=$(comm -12 "$PREVIOUS_POD_NAMES_FILE" "$CURRENT_POD_NAMES_FILE")
          rm "$PREVIOUS_POD_NAMES_FILE"
          rm "$CURRENT_POD_NAMES_FILE"

          # Can't simply copy associative arrays in bash, so perform maintenance on PREVIOUS_POD_REPORTS as we go.
          for POD_NAME in ${REMOVED_POD_NAMES}; do
              log "Pod change detected: ${POD_NAME} removed."
              unset PREVIOUS_POD_REPORTS["$POD_NAME"]
          done
          for POD_NAME in ${NEW_POD_NAMES}; do
              log "Pod change detected: ${POD_NAME}: ${POD_REPORTS["${POD_NAME}"]}.";
              PREVIOUS_POD_REPORTS["$POD_NAME"]=${POD_REPORTS["${POD_NAME}"]}
          done;
          for POD_NAME in ${EXISTING_POD_NAMES}; do
              # The hostname won't change, so only report the pod status. ${VAR%% *} means delete everything after the first space
              # space. Thus "<pod_name> <pod_status> on host <nodeName>" becomes "<pod_name>"
              # http://tldp.org/LDP/abs/html/string-manipulation.html
              OLD_REPORT="${PREVIOUS_POD_REPORTS[${POD_NAME}]}"
              NEW_REPORT="${POD_REPORTS[${POD_NAME}]}"
              if [[ "${OLD_REPORT}" != "${NEW_REPORT}" ]]; then
                  log "Pod change detected: ${POD_NAME} changed to ${NEW_REPORT%% *} (previously ${OLD_REPORT%% *})."
                  PREVIOUS_POD_REPORTS["$POD_NAME"]="${NEW_REPORT}"
              fi
          done;
      }


      # ------ Initialize ---------
      log_heading "Beginning $APP_NAME test"
      # Sanity-check kubeapi connectivity
      kcfw_log cluster-info

      # ------ Clean up prior runs ---------
      log "Cleaning up SparkApplication/Pod older than 1 hours from prior runs."
      # https://stackoverflow.com/questions/48934491/kubernetes-how-to-delete-pods-based-on-age-creation-time
      APPS=$(kcfw get sparkapplication -o go-template='{{range .items}}{{.metadata.name}} {{.metadata.creationTimestamp}}{{"\n"}}{{end}}' | awk '$2 <= "'$(date -d'now-1 hours' -Ins --utc | sed 's/+0000/Z/')'" { print $1 }')
      for APP in ${APPS}; do
          PODSELECTOR="sparkoperator.k8s.io/app-name=${APP}"
          kcfw_log delete sparkapplication ${APP}
          kcfw_log delete pod -l ${PODSELECTOR}
      done

      # ------ Run ---------
      log "Creating SparkApplication $APP_NAME"
      kcfw_log create -f $SPEC
      SPARK_APP_START_TIME=$(epoch)

      # If we've gotten this far, we'd like to collect as much forensic data as possible
      set +o errexit

      LAST_LOGGED=$(epoch)
      log "Waiting for SparkApplication $APP_NAME to reach a terminal state."
      STATE=$(state)
      while true; do
          EPOCH=$(epoch)
          if $(echo ${STATE} | grep -P '(COMPLETED|FAILED)' > /dev/null); then
              log "SparkApplication $APP_NAME has terminated after $(($EPOCH - $SPARK_APP_START_TIME)) seconds. State is $STATE."
              break
          fi
          # Use start time of script for timeout computation in order to still exit in timely fashion even if setup was slow
          if (( EPOCH - START_TIME >= TIMEOUT_SECS )); then
              log "Timeout reached. Aborting wait for SparkApplication $APP_NAME even though in non-terminal state $STATE."
              break
          fi
          if (( EPOCH - LAST_LOGGED > 60 )); then
              log "...still waiting for terminal state (currently $STATE) after $((EPOCH-SPARK_APP_START_TIME)) seconds.";
              events;
              LAST_LOGGED=${EPOCH}
          fi;
          report_pod_changes
          sleep 1;
          STATE=$(state)
      done;
      EXIT_CODE=$(echo ${STATE} | grep COMPLETED > /dev/null; echo $?)

      # ------ Report Results ---------
      report_pod_changes
      events

      POD_NAME=$(kcfw get pod -l ${SELECTOR},spark-role=driver -o name)
      if [[ -z ${POD_NAME} ]]; then
          log "Cannot locate driver pod. Maybe it never started? No logs to display."
      else
          pod_log ${POD_NAME}
      fi

      log -------- Executor Pods ----------
      EXECUTOR_PODS=$(kcfw get pod -l ${SELECTOR},spark-role=executor -o name)
      for POD_NAME in ${EXECUTOR_PODS}; do
          pod_log ${POD_NAME}
      done;

      if $(echo ${STATE} | grep -P '(COMPLETED|FAILED)' > /dev/null); then
          # Delete so that Kubernetes is in a cleaner state when the next test execution starts
          log "Cleaning up stuff for complated or failed test."
          kcfw_log delete sparkapplication ${APP_NAME}
          kcfw_log delete pod -l ${SELECTOR}
      fi

      log_heading "Completion of $APP_NAME test, returning $EXIT_CODE"
      exit ${EXIT_CODE}
    kubeconfig-impersonation-proxy: |
      apiVersion: v1
      clusters:
      - cluster:
          certificate-authority: /certs/ca.pem
          server: https://kubernetes-api-test-flowsnake-prd.slb.sfdc.net
        name: kubernetes
      contexts:
      - context:
          cluster: kubernetes
          user: kubernetes
        name: default-context
      current-context: default-context
      kind: Config
      preferences: {}
      users:
      - name: kubernetes
        user:
          client-certificate: /certs/client/certificates/client.pem
          client-key: /certs/client/keys/client-key.pem
  kind: ConfigMap
  metadata:
    name: watchdog-spark-on-k8s-script-configmap
    namespace: flowsnake
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    labels:
      name: watchdog-spark-operator
    name: watchdog-spark-operator
    namespace: flowsnake
  spec:
    replicas: 10
    selector:
      matchLabels:
        app: watchdog-spark-operator
        apptype: monitoring
    template:
      metadata:
        annotations:
          madkub.sam.sfdc.net/allcerts: '{"certreqs": [{"cert-type": "client", "kingdom":
            "prd", "name": "watchdogsparkoperator", "role": "flowsnake_test.flowsnake-watchdog"}]}'
        labels:
          app: watchdog-spark-operator
          apptype: monitoring
          flowsnakeOwner: dva-transform
          flowsnakeRole: WatchdogSparkOperator
      spec:
        containers:
        - command:
          - /sam/watchdog
          - -role=CLI
          - -emailFrequency=1h
          - -timeout=2s
          - -funnelEndpoint=ajna0-funnel1-0-prd.data.sfdc.net:80
          - --config=/config/watchdog.json
          - -cliCheckerCommandTarget=SparkOperatorTest
          - --hostsConfigFile=/sfdchosts/hosts.json
          - -watchdogFrequency=1m
          - -alertThreshold=1m
          - -cliCheckerTimeout=6m
          - -includeCommandOutput=true
          env:
          - name: DOCKER_TAG
            value: "4"
          - name: S3_PROXY_HOST
            value: public0-proxy1-0-prd.data.sfdc.net
          - name: DRIVER_SERVICE_ACCOUNT
            value: spark-driver-flowsnake-watchdog
          - name: DOCKER_REGISTRY
            value: ops0-artifactrepo2-0-prd.data.sfdc.net
          image: ops0-artifactrepo2-0-prd.data.sfdc.net/dva/flowsnake-spark-on-k8s-integration-test-runner:4
          imagePullPolicy: IfNotPresent
          name: watchdog
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: "1"
              memory: 1Gi
          volumeMounts:
          - mountPath: /config
            name: config
          - mountPath: /sfdchosts
            name: sfdchosts
          - mountPath: /watchdog-spark-scripts
            name: watchdog-spark-scripts
          - mountPath: /certs
            name: datacerts
        - args:
          - /sam/madkub-client
          - --madkub-endpoint
          - https://10.254.208.254:32007
          - --maddog-endpoint
          - https://all.pkicontroller.pki.blank.prd.prod.non-estates.sfdcsd.net:8443
          - --maddog-server-ca
          - /etc/pki_service/ca/security-ca.pem
          - --madkub-server-ca
          - /etc/pki_service/ca/cacerts.pem
          - --token-folder
          - /tokens
          - --kingdom
          - prd
          - --superpod
          - None
          - --estate
          - prd-data-flowsnake_test
          - --refresher
          - --run-init-for-refresher-mode
          - --cert-folders
          - watchdogsparkoperator:/certs
          - --ca-folder
          - /etc/pki_service/ca
          - --funnel-endpoint
          - http://ajna0-funnel1-0-prd.data.sfdc.net:80
          env:
          - name: MADKUB_NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MADKUB_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MADKUB_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ops0-artifactrepo2-0-prd.data.sfdc.net/docker-release-candidate/tnrp/sam/madkub:1.0.0-0000084-9f4a6ca6
          name: sam-madkub-integration-refresher
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 7337
          volumeMounts:
          - mountPath: /certs
            name: datacerts
          - mountPath: /tokens
            name: tokens
          - mountPath: /etc/pki_service/ca
            name: certificate-authority
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/certificates
            name: client-certificate
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/keys
            name: client-key
            readOnly: true
        hostNetwork: true
        initContainers:
        - args:
          - /sam/madkub-client
          - --madkub-endpoint
          - https://10.254.208.254:32007
          - --maddog-endpoint
          - https://all.pkicontroller.pki.blank.prd.prod.non-estates.sfdcsd.net:8443
          - --maddog-server-ca
          - /etc/pki_service/ca/security-ca.pem
          - --madkub-server-ca
          - /etc/pki_service/ca/cacerts.pem
          - --token-folder
          - /tokens
          - --kingdom
          - prd
          - --superpod
          - None
          - --estate
          - prd-data-flowsnake_test
          - --cert-folders
          - watchdogsparkoperator:/certs
          - --ca-folder
          - /etc/pki_service/ca
          - --funnel-endpoint
          - http://ajna0-funnel1-0-prd.data.sfdc.net:80
          env:
          - name: MADKUB_NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MADKUB_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MADKUB_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: ops0-artifactrepo2-0-prd.data.sfdc.net/docker-release-candidate/tnrp/sam/madkub:1.0.0-0000084-9f4a6ca6
          name: sam-madkub-integration-init
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 7337
          volumeMounts:
          - mountPath: /certs
            name: datacerts
          - mountPath: /tokens
            name: tokens
          - mountPath: /etc/pki_service/ca
            name: certificate-authority
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/certificates
            name: client-certificate
            readOnly: true
          - mountPath: /etc/pki_service/platform/platform-client/keys
            name: client-key
            readOnly: true
        restartPolicy: Always
        serviceAccount: watchdog-spark-operator-serviceaccount
        serviceAccountName: watchdog-spark-operator-serviceaccount
        volumes:
        - configMap:
            name: watchdog
          name: config
        - configMap:
            defaultMode: 493
            name: watchdog-spark-on-k8s-script-configmap
          name: watchdog-spark-scripts
        - configMap:
            name: sfdchosts
          name: sfdchosts
        - emptyDir:
            medium: Memory
          name: datacerts
        - emptyDir:
            medium: Memory
          name: tokens
        - hostPath:
            path: /etc/pki_service/ca
          name: certificate-authority
        - hostPath:
            path: /etc/pki_service/platform/platform-client/certificates
          name: client-certificate
        - hostPath:
            path: /etc/pki_service/platform/platform-client/keys
          name: client-key
kind: List
metadata: {}
